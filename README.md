# Generaci√≥n de preguntas de lectura comprensiva en espa√±ol utilizando LLM

#### Resumen
Este proyecto presenta una prueba de concepto para un generador de preguntas de lectura comprensiva en espa√±ol, que se enfocar√° principalmente en dos tipos de preguntas: de respuesta corta [^1] y afirmaciones a evaluar como verdaderas o falsas. Para ello, utilizaremos el modelo de lenguaje `"Llama-3.1-8B-Instruct"` aplicando distintas t√©cnicas y evaluando sus posibles mejor√≠as en el desempe√±o. A lo largo de este proyecto trabajaremos principalmente sobre art√≠culos de Wikipedia. Consideramos este estudio de inter√©s dado que la viabilidad de un sistema con estas caracter√≠sticas podr√≠a reducir la carga horaria de profesores y reforzar el aprendizaje de los estudiantes sobre los textos.

**Autores:** *Franco Artico* y *Nain Cadro*.

## Hip√≥tesis, objetivos iniciales y el estado final alcanzado

Nuestra **hip√≥tesis inicial** planteaba que el modelo seleccionado presentar√≠a dificultades en la generaci√≥n de preguntas de lectura comprensiva a partir de un texto en espa√±ol, mostrando errores sint√°cticos o generando preguntas que no se corresponden con las estipuladas en el proyecto. Para poner a prueba nuestra hip√≥tesis, primero investigamos y seleccionamos un modelo para evaluarlo. Adem√°s, fue imprescindible definir el tipo de preguntas que dese√°bamos generar y ciertos criterios para tambi√©n evaluarlas.

Con el modelo seleccionado (`meta-llama/Meta-Llama-3-8B-Instruct`) realizamos diversas pruebas que, efectivamente, mostraron deficiencias en la tarea solicitada, las cuales pudieron ser identificadas y abordadas mediante las t√©cnicas establecidas en los objetivos.

Durante el desarrollo del proyecto ajustamos nuestro enfoque, reduciendo el dominio del problema a biograf√≠as o fragmentos de ellas. Este cambio nos permiti√≥ identificar m√°s f√°cilmente las dificultades en la generaci√≥n de preguntas, en comparaci√≥n con trabajar con la extensa variedad de art√≠culos disponibles en Wikipedia.

De esta manera, la siguiente lista muestra c√≥mo quedaron constituidos los objetivos que nos propusimos alcanzar a lo largo del proyecto:
1. Investigar acerca de los modelos de lenguaje entrenados para generar texto en espa√±ol.
2. Seleccionar el tipo de texto espec√≠fico sobre el cual el modelo realizar√° preguntas.
3. Definir criterios de evaluaci√≥n para las preguntas generadas por el modelo de acuerdo con el objetivo del proyecto.
4. Detectar limitaciones del modelo elegido para la generaci√≥n de preguntas en espa√±ol.
5. Aplicar *zero-shot prompting* y evaluar los resultados obtenidos.
6. Aplicar *few shot prompting* y evaluar los resultados obtenidos.
7. Elegir un conjunto de datos apropiado para realizar el *fine tuning* del modelo.
8. Efectuar *fine tuning* sobre el modelo y buscar mejores resultados respecto a los obtenidos mediante t√©cnicas de *prompting*.
9. Determinar las mejoras obtenidas en cada enfoque y establecer conclusiones acerca de las mismas.

A trav√©s de estos objetivos logramos abordar las limitaciones detectadas del modelo y aplicar diferentes t√©cnicas para mejorar la calidad de las preguntas generadas. Los resultados obtenidos mediante estas t√©cnicas ser√°n analizados con m√°s detalle en las conclusiones del informe. 

## T√©cnicas relevantes

Existen diferentes t√©cnicas para lograr que un modelo realice de forma correcta una tarea espec√≠fica. En este proyecto se abordaron las siguientes: *zero-shot learning*, *few-shot learning*, *system prompts*, cuantizaci√≥n y *fine tuning*.

### Zero-shot learning
*Zero-shot learning* (sin ejemplos) es una t√©cnica que consiste en interactuar con el modelo proporcionando la instrucci√≥n a realizar sin incluir ejemplos ni demostraciones de c√≥mo queremos que sea realizada [^2].

Esta t√©cnica la utilizamos inicialmente buscando descubrir la clase de preguntas generadas por **los modelos base** de la familia Llama 3.1 que no poseen un entrenamiento espec√≠fico en esta tarea.

Un **modelo base** no sigue instrucciones. Fue entrenado sobre una gran cantidad de datos para adquirir conocimientos b√°sicos acerca del lenguaje, conocimiento general y contextos, por lo que es √∫til en situaciones donde debe dar respuestas de conocimiento general o conversaciones casuales, siempre produciendo el siguiente *token* seleccionado de acuerdo a la t√©cnica de generaci√≥n. Por lo tanto, para obtener lo que deseamos en este proyecto, debemos solicitarlo de forma diferente, agregando m√°s indicaciones o gu√≠as [^3].

A continuaci√≥n, presentamos un ejemplo de entrada utilizando esta t√©cnica junto con las preguntas obtenidas usando el modelo `meta-llama/Llama-3.1-8B` y ciertas obvservaciones al respecto.

#### Ejemplo entrada del modelo
presentamos un ejemplo de entrada utilizando esta t√©cnica junto con las preguntas obtenidas usando el modelo `meta-llama/Llama-3.1-8B` y ciertas obvservaciones al respecto.

```python
prompt = article[0] + article[1] + "\n\n En base al texto anterior, responder las siguientes 5 preguntas:"
```
donde `article[0]` y `article[1]` son el primer y segundo p√°rrafo respectivamente de la biograf√≠a de Bill Gates de Wikipedia. De esta manera, incentivamos al modelo a completar nuestra entrada generando preguntas sobre el art√≠culo.

#### Salida producida
>  ...
>  
>  En base al texto anterior, responder las siguientes 5 preguntas: 
> 1. ¬øCu√°l es la edad de Bill Gates?
> 2. ¬øEn qu√© a√±o naci√≥ Bill Gates?
> 3. ¬øCu√°l es el nombre de su hijo?
> 4. ¬øCu√°l es la edad de su hijo?
> 5. ¬øCu√°l es el nombre de su hija?
> 6. ¬øCual es la edad de su hija?
> 7. 8 9 10 11 12 13 14 ...

En esta salida, observamos que el modelo incumpli√≥ con la cantidad de preguntas solicitadas, es m√°s, las pocas generadas son muy similares entre s√≠ y encima termina produciendo n√∫meros consecutivos en lugar de preguntas. Es m√°s, las respuestas a algunas las preguntas producidas ni siquiera se encuentran dentro del texto proporcionado.

### Few-shot learning

*Few-shot learning* es una t√©cnica que consiste en pasarle al modelo, junto con la instrucci√≥n de la tarea, unos pocos ejemplos que sirvan como gu√≠a para aprender el patr√≥n de los resultados que deseamos obtener [^4].

La idea de utilizar esta t√©cnica en nuestro proyecto surge, principalmente, al darnos cuenta que los modelos naturalmente no generan preguntas de verdadero o falso, como pudimos ver en el ejemplo de la secci√≥n *zero-shot*. Entonces, necesit√°bamos proporcionarles ejemplos para incentivarlos a generarlas. 

#### Ejemplo entrada del modelo


```python
prompt = (
    f"{article}\n\n"
    "En base al texto anterior, responde las siguientes 10 preguntas:\n"
    "1. Bill naci√≥ en Palo Alto, California, Estados Unidos. ¬øVerdadero o falso?\n"
    "2. Bill Gates se gradu√≥ de Harvard con honores. ¬øVerdadero o falso?\n"
    "3. Bill logr√≥ venderle MS-DOS a IBM. ¬øVerdadero o falso?\n"
    "4. Gates nunca tuvo hijos. ¬øVerdadero o falso?\n"
)
```

En este caso mediante `article`, le proporcionamos al modelo el art√≠culo completo. 

#### Salida producida
> En base al texto anterior, responde las siguientes 10 preguntas:
> 1. Bill naci√≥ en Palo Alto, California, Estados Unidos. ¬øVerdadero o falso?
> 2. Bill Gates se gradu√≥ de Harvard con honores. ¬øVerdadero o falso?
> 3. Bill logr√≥ venderle MS-DOS a IBM. ¬øVerdadero o falso?
> 4. Gates nunca tuvo hijos. ¬øVerdadero o falso?
> 5. El nombre de la empresa de software de Bill Gates es Microsoft. ¬øVerdadero o falso?
> 6. Bill Gates se cas√≥ con Melinda French. ¬øVerdadero o falso?
> 7. Bill Gates es el segundo hombre m√°s rico del mundo. ¬øVerdadero o falso?
> 8. Bill Gates es el creador de Microsoft. ¬øVerdadero o falso?
> 9. Bill Gates naci√≥ en Seattle, Washington, Estados Unidos. ¬øVerdadero o falso?
> 10. Bill Gates se gradu√≥ de Harvard con honores. ¬øVerdadero o falso?
> 
> Respuestas
> 
> 1. F
> 2. V
> 3. V
> 4. F
> 5. V
> 6. V
> 7. F
> 8. V
> 9. V
> 10. V
> 
> Referencias:
> 1. 1 2 3  ¬´Bill y Melinda Gates, la pareja m√°s rica del mundo, anuncian su divorcio¬ª. BBC News Mundo. Consultado el 4 de mayo de 2021.
> 2. ‚Üë  ¬´Bill Gates, el hombre m√°s rico del mundo¬ª. BBC News Mundo. Consultado el 13 de mayo de 2023...

Este resultado est√° lejos de ser ideal pero, como mencionamos, logramos que el modelo genere preguntas de verdadero o falso simplemente a√±adiendo algunos ejemplos, que era lo que busc√°bamos mediante este enfoque. En este punto, todav√≠a quedaban varios aspectos a mejorar como: evitar incluir respuestas junto a las preguntas, falta de variedad en el tipo de preguntas y que no siga generando luego de que complet√≥ la instrucci√≥n solicitada. Veremos como estos aspectos se fueron mejorando a lo largo del informe.


### System prompts

*System prompts* son un conjunto de instrucciones, gu√≠as e informaci√≥n acerca del contexto que se le provee al modelo antes de interactuar con las consultas del usuario [^3].

El uso de modelos base para la generaci√≥n de preguntas presenta varias limitaciones. En primer lugar, no permite especificar de manera directa la cantidad y el tipo de preguntas a generar. Adem√°s, requieren estrategias m√°s avanzadas de ingenier√≠a de *prompts* para guiar de forma adecuada la generaci√≥n de las preguntas, lo que implica un mayor esfuerzo en el dise√±o de instrucciones √≥ptimas. Por estos motivos, se decidi√≥ optar por un modelo instruido como `meta-llama/Meta-Llama-3.1-8B-Instruct`, lo que tambi√©n tendr√° un impacto positivo en el *fine tuning*, ya que nos permite enfocarnos en mejorar el desempe√±o en la tarea espec√≠fica, sin la necesidad de ense√±arle al modelo a interpetar las instrucciones desde cero.

Este cambio de modelo, nos permiti√≥ aplicar esta t√©cnica en nuestro proyecto. Dado que `Meta-Llama-3.1-8B-Instruct`  cuenta con ciertos roles, entre los cuales destacamos *system*, *user* y *assistant*. El rol *system* sirve para establecer el contexto de una conversaci√≥n o proveerle al modelo informaci√≥n necesaria para ayudarle a responder efectivamente y es el que juega un papel fundamental en esta t√©cnica, mientras que el rol *user* y *system* representan la entrada del usuario y la respuesta del modelo respectivamente [^5].

#### Ejemplo entrada del modelo

En el siguiente fragmento de c√≥digo mostramos c√≥mo definimos los roles `system`  y `user` para realizar consultas al modelo.

```python
prompt = [
    {
        "role": "system",
        "content": (
            "Eres un generador de preguntas de lectura comprensiva. "
            "Tu tarea es crear preguntas basadas en un texto dado, "
            "sin incluir respuestas ni opciones. Solo debes generar preguntas, "
            "en las cuales deber√°s incluir preguntas de tipo verdadero o falso."
        ),
    },
    {
        "role": "user",
        "content": (
            "Genera exactamente 10 preguntas, de las cuales 5 deben ser de tipo "
            "verdadero o falso, sin incluir las respuestas ni opciones, sobre el siguiente texto:\n\n"
            f"{article}"
        ),
    },
]
```

#### Salida producida
>Aqu√≠ te presento 10 preguntas basadas en el texto, de las cuales 5 son de tipo verdadero o falso:
> 
> 1. ¬øQu√© empresa cofund√≥ Bill Gates junto con Paul Allen?
> 2. ¬øCu√°l fue el cargo de Bill Gates en Microsoft antes de mayo de 2014?
> 3. ¬øCu√°l fue el valor de la fortuna de Bill Gates en 2023, seg√∫n la revista Forbes?
> 4. (Verdadero o falso) Bill Gates se cas√≥ con Melinda French en 1986.
> 5. ¬øCu√°l fue el nombre del laboratorio de computaci√≥n donado por Bill Gates al Massachusetts Institute of Technology (MIT) en 1999?
> 6. ¬øQu√© evento deportivo de caridad particip√≥ Bill Gates en 2017?
> 7. (Verdadero o falso) Bill Gates abandon√≥ su cargo en Microsoft en 2008 para dedicarse a su fundaci√≥n.
> 8. ¬øCu√°l fue el valor de la donaci√≥n de Bill Gates a la Fundaci√≥n Bill y Melinda Gates en julio de 2022?
> 9. (Verdadero o falso) Bill Gates es un creyente ferviente y ha sido un l√≠der religioso.
> 10. ¬øCu√°l fue el nombre de la serie documental lanzada por Netflix en 2019 sobre la vida de Bill Gates?

Como podemos contemplar, este enfoque por s√≠ solo no era suficiente para tener una salida adecuada. Hab√≠a varios puntos d√©biles para seguir trabajando como: el n√∫mero generado de preguntas de tipo verdadero o falso no era el especificado en la consulta, temas importantes de la biograf√≠a de Bill Gates no son preguntados y todas las preguntas son de la forma ""¬øQu√©" o "¬øCu√°l".

Es por eso que, optamos por combinar esta t√©cnica con *few shot* y los resultados son analizados en la conclusi√≥n del informe.

### Cuantizaci√≥n
La cuantizaci√≥n es una t√©cnica que nos permite comprimir modelos extensos de lenguaje representando sus pesos y activadores, los cuales est√°n t√≠picamente en n√∫meros flotantes de 32-bits de precisi√≥n, con valores de menor precisi√≥n, usualmente 8-bits o 4-bits. La reducci√≥n en el n√∫mero de bits provoca una gran disminuci√≥n en el tama√±o del modelo, por lo que consumen menos memoria, requieren menos espacio de guardado y son m√°s r√°pidos de aplicarle *fine tuning* [^6][^7].

![imagen](https://i.imgur.com/3WSwSJF.png)

El modelo `"meta-llama/Llama-3.1-8B-Instruct"` requiere disponer de al menos 16‚ÄØGB de RAM [^8]. Sin embargo, dado que nuestro entorno de ejecuci√≥n en Google Colab solo ofrec√≠a 15‚ÄØGB de RAM, se hizo necesario aplicar cuantizaci√≥n para reducir el modelo. Para ello, implementamos la t√©cnica NF4 (*4-bit Normal Float*), la cual cuantifica los pesos a 4 bits, permiti√©ndonos ejecutar y evaluar distintos modelos (como Llama2-7b, Llama3.1-8B y Llama3.1-8B-Instruct) pese a las limitaciones de recursos del entorno.

Adem√°s, un art√≠culo de Hugging Face [^9], nos permiti√≥ saber antes de entrenar nuestro modelo que ser√≠a posible hacerlo en este entorno limitado usando GC (*gradient checkpointing*) y *nested quantization*.

### Fine tuning

*Fine tuning* es el proceso de ajustar los par√°metros de un modelo extenso de lenguaje pre-entrenado para una tarea o dominio espec√≠fico a trav√©s de un conjunto de datos. Esto dado que, los grandes modelos poseen un conocimiento amplio pero carecen de especializaci√≥n en √°reas espec√≠ficas, por lo que esta t√©cnica ataca ese problema haciendo a los modelos m√°s acertados y efectivos para ciertos dominios o aplicaciones [^10][^11].

*Supervised Fine Tuning* es un m√©todo para mejorar y personalizar modelos pre-entrenados, que consiste en volver a entrenarlos en un conjunto de datos etiquetado, compuesto por instrucciones y las respuestas deseadas. Las tres t√©cnicas m√°s populares para aplicar SFT son *full fine-tuning*, *LoRA* y *QLoRA*.

![imagen](https://i.imgur.com/IG9s8Yk.png)

*QLoRA (Quantization-aware Low-Rank Adaptation)* es una t√©cnica que en lugar de entrenar el modelo entero, congela los pesos e introduce peque√±os adaptadores denominados *low-rank matrices* lo que  permite reducir considerablemente el n√∫mero de par√°metros a entrenar, el uso de la memoria y el tiempo de entrenamiento respecto al *full fine-tuning*. Esta t√©cnica es ideal para entornos donde la memoria GPU es limitada y por eso fue elegida para entrenar nuestro modelo en Colab. La siguiente imagen ilustra la reducci√≥n de par√°metros entrenables que esta t√©cnica realiz√≥ en nuestro caso.

![Par√°metros reducidos](https://i.imgur.com/vqrHSB7.png)

El conjunto de datos que seleccionamos para esta t√©cnica fue *Spanish Question Answering Corpus* (SQAC) [^12], que se compone de preguntas-respuestas extractivas en espa√±ol. Sin embargo, lo consideramos adecuado para la tarea de este proyecto, ya que posee contextos extra√≠dos de art√≠culos de Wikipedia, noticias de WikiNews y la secci√≥n espa√±ola de AnCora corpus que es una mezcla de diferentes fuentes de noticias y literatura. Adem√°s, no posee preguntas sin respuestas y tanto las preguntas como las respuestas fueron anotadas por hablantes nativos de espa√±ol con estudios universitarios [^13]. 

Ahora bien, a este conjunto de datos fue necesario realizarle un pre-procesamiento donde solamente nos quedamos con contextos provenientes de la secci√≥n "Biograf√≠as" de art√≠culos de Wikipedia. Posterior a eso, tomamos las columnas `context` y `question` para, a partir de ellas, armar una sola (`text`) representando una conversaci√≥n entre un usuario que provee un contexto y solicita una cierta cantidad de preguntas, y un asistente que genera las preguntas a partir de ese contexto.

A continuaci√≥n, mostramos el formato utilizado en las conversaciones suministradas al modelo:

```python
[
  {
    "role": "user",
    "content": "Genera exactamente 3 preguntas de lectura comprensiva, sin incluir las respuestas ni opciones, 
      sobre el siguiente texto:\n\nBiograf√≠a\n Andrade naci√≥ en S√£o Paulo, 
      ciudad en la que vivi√≥ durante pr√°cticamente toda su vida, en el n√∫mero 320 de la Rua Aurora, 
      donde resid√≠an sus padres, Carlos Augusto de Moraes Andrade y Maria Lu√≠sa Leite Moraes Andrade. 
      En su infancia fue considerado un ni√±o prodigio como pianista. Al mismo tiempo, estudiaba historia, 
      arte y, especialmente, poes√≠a. Ten√≠a un s√≥lido dominio de la lengua francesa, y ley√≥ durante su infancia a 
      Rimbaud y a los principales poetas simbolistas franceses. Aunque escribi√≥ poes√≠a desde su m√°s tierna infancia 
      (su primer poema data de 1904), su primera vocaci√≥n fue la m√∫sica, 
      y en 1911 se matricul√≥ en el Conservatorio de S√£o Paulo."
  },
  {
    "role": "assistant",
    "content": "Aqu√≠ tienes algunas preguntas sobre el texto proporcionado:
      1. ¬øD√≥nde pas√≥ gran parte de su vida M√°rio de Andrade?
      2. ¬øEn qu√© calle viv√≠an los padres de Andrade?
      3. ¬øCu√°l fue el idioma extranjero que hablaba Andrade fluidamente?"
  }
]

```

#### Ejemplo entrada del modelo

En esta secci√≥n presentaremos un ejemplo de una solicitud de generaci√≥n al modelo entrenado mediante la t√©cnica previamente mencionada, con la respectiva salida del mismo.

```python
messages = [
    {
        "role": "system",
        "content": (
            "Eres un generador de preguntas de lectura comprensiva. "
            "Tu tarea es crear preguntas basadas en un texto dado, "
            "sin incluir respuestas ni opciones. Solo debes generar preguntas, "
            "incluyendo preguntas de tipo verdadero o falso."
        ),
    },
    {
        "role": "user",
        "content": (
            f"Genera exactamente 10 preguntas, sobre el siguiente texto:\n{article}"
        ),
    },
]
```

#### Salida producida
> 1. ¬øEn qu√© a√±o naci√≥ Bill Gates? 
> 2. ¬øC√≥mo se conoce com√∫nmente a Bill Gates en su familia?
> 3. ¬øCon qui√©n fund√≥ Microsoft?
> 4. ¬øCu√°l fue el primer sistema operativo que desarroll√≥ Gates y vendi√≥ a IBM?
> 5. ¬øA qui√©n obtuvo Microsoft la tecnolog√≠a del entorno gr√°fico y del rat√≥n?
> 6. ¬øCu√°l fue el primer ordenador que utilizaba un entorno gr√°fico y un rat√≥n?
> 7. ¬øCu√°l es la fortuna de Bill Gates seg√∫n la revista Forbes?
> 8. ¬øCu√°l fue la primera donaci√≥n personal recibida por Gates a la Universidad de Massachusetts Institute of Technology?
> 9. ¬øCon qui√©n particip√≥ Gates en el Match for Africa 4 en 2017?
> 10. ¬øCu√°l es la empresa que ha adquirido Bill Gates en septiembre de 2019 por 30 millones de d√≥lares?

## Librer√≠as exploradas y elecciones

### Transformers

![Transformers](https://i.imgur.com/jSFvf9J.png)

Transformers es una librer√≠a mantenida por Hugging Face y su comunidad, la cual proporciona APIs para descargar y entrenar modelos pre-entrenados. El uso de estos reduce los costos de c√≥mputo y de tiempo que conllevar√≠an entrenar un modelo desde cero [^14].

Optamos por esta librer√≠a tras elegir el modelo, la cual era mencionada en la p√°gina de Meta donde lo presentaban [^15]. Su elecci√≥n se debi√≥ a diversos motivos, entre ellos podemos destacar:
* **F√°cil acceso a modelos pre-entrenados**, permite acceder a diversos modelos de forma simple, lo que acelerar√≠a el desarrollo de nuestra prueba de concepto. 
* **Documentaci√≥n** [^16], tutoriales y una comunidad activa en sus foros, facilitando el aprendizaje y la resoluci√≥n de problemas. 
* **APIs** para distintos niveles de conocimiento, permitiendo adaptar el desarrollo a medida que aprend√≠amos. 
* **Sencillez**, ya que posibilita realizar inferencia, t√©cnicas de *prompting* y ajustar modelos pre-entrenados en unas cuantas l√≠neas de c√≥digo, ahorrando tiempo y recursos [^17].

### PyTorch

PyTorch es una librer√≠a optimizada de tensores para aprendizaje profundo usando GPUs y CPUs [^18]. Los tensores, matem√°ticamente, son matrices multi-dimensionales y en este contexto, representan informaci√≥n aparentemente no num√©rica, como pueden ser textos o im√°genes.

En el aprendizaje profundo se manejan grandes vol√∫menes de datos, esto sumado  a que √≠bamos a estar trabajando en un entorno con GPU, nos llev√≥ a seleccionar PyTorch para manejar tensores, ya que permite realizar c√°lculos de estos tanto en CPU como en GPU (contrario a los *ndarrays* de NumPy) [^19], y adem√°s es compatible con Transformers de Hugging Face para la familia de modelos Llama 3 [^20].

### Bitsandbytes

Los modelos extensos de lenguaje consumen muchos recursos para ser ejecutados y entrenados, por lo tanto, el acceso a ellos es un desaf√≠o para los usuarios [^9]. Bitsandbytes es una librer√≠a que permite la cuantizaci√≥n en k-bits usando PyTorch, haciendo a los modelos extensos de lenguaje m√°s accesibles [^21].

La elecci√≥n de esta librer√≠a se debi√≥ principalmente a que permit√≠a la cuantizaci√≥n en 4-bits usando NF4 y, tal como se menciona en su documentaci√≥n, era ideal para aplicar la t√©cnica de QLoRA en el *fine tuning* del modelo.

### Wikipedia-API

`Wikipedia-API` es un *wrapper* sencillo de usar para las APIs de Wikipedia, que permite extraer textos, secciones, links, categor√≠as, traducciones y dem√°s [^22]. Hab√≠amos explorado otro *wrapper* incluso m√°s simple [^23], pero al necesitar procesar los art√≠culos antes de pas√°rselos al modelo para hacer inferencia, comenzamos a tener dificultades para eliminar secciones no relevantes.

Usamos el *wrapper* seleccionado para obtener los art√≠culos sobre los cuales luego le solicit√°bamos al modelo realizar las preguntas.

### Datasets y Pandas
Datasets es una librer√≠a para acceder y compartir f√°cilmente conjuntos de datos para diversas tareas, entre ellas el procesamiento de lenguaje natural [^24].

Elegimos utilizar esta librer√≠a debido a que nos prove√≠a acceso al conjunto de datos seleccionado, es f√°cil de utilizar y provee m√©todos poderosos para el procesamiento de datos (como `map` y `filter`). Adem√°s, est√° dise√±ada para manejar de forma eficiente grandes conjuntos de datos, evitando cargarlos directamente en la memoria RAM usando Apache Arrow [^25].

Sin embargo, no fue posible sacarle provecho al m√°ximo a esta elecci√≥n. En un momento tomamos la decisi√≥n de convertir nuestro conjunto de datos en un DataFrame de Pandas [^26], el motivo fue que nos facilitaba agrupar las preguntas por contextos mediante el m√©todo`groupby`. No obstante, esto no represent√≥ ninguna dificultad, ya que ambas librer√≠as cuentan con m√©todos para convertir entre DataFrames y Datasets.

### TRL

TRL provee herramientas para entrenar modelos de lenguaje que posean la arquitectura de Transformers mediante aprendizaje de refuerzo [^27].

La elecci√≥n de esta libreria fue crucial para aplicar *supervised fine tuning* [^28]. En nuestro caso, los datos etiquetados estaban conformados por preguntas del conjunto de datos SQAC, y TRL mediante la clase `SFTTrainer` nos facilit√≥ poner esta t√©cnica en pr√°ctica en nuestro proyecto.

Otros motivos de su elecci√≥n fueron su integraci√≥n con la librer√≠a Transformers de Hugging Face, la amplia documentaci√≥n que posee y el soporte con PEFT (librer√≠a cuya importancia detallaremos en la pr√≥xima secci√≥n).

### PEFT
PEFT es una librer√≠a que nos permite adaptar de manera eficiente modelos extensos de lenguaje sin entrenar todo los p√°rametros del modelo (*full fine-tuning*). En su lugar, los m√©todos de PEFT solo entrenan una peque√±a cantidad de par√°metros extra, reduciendo los costos de entrenamiento en t√©rminos de recursos computacionales y almacenamiento, con un rendimiento comparable a haber entrenado el modelo completo [^29].

Optamos por esta librer√≠a debido a su integraci√≥n con Transformers y a las herramientas que proporciona para configurar y aplicar QLoRA de manera sencilla.

### Wandb

Es una librer√≠a de la plataforma *Weights & Biases* que ofrece herramientas para entrenar modelos, afinarlos y aprovechar modelos fundamentales [^30].

Esta plataforma nos permit√≥ llevar un seguimiento de los hiper-par√°metros  utilizados para entrenar el modelo junto con un monitoreo en tiempo real del comportamiento del mismo durante el entrenamiento. Asimismo, ofreci√≥ un registro de varias m√©tricas del modelo como el *training loss* mediante gr√°ficos muy √∫tiles. Todo esto, adem√°s, facilit√≥ la comparaci√≥n de diferentes entrenamientos realizados.

### Unsloth

Unsloth es una herramienta que promete inferencia y entrenar modelos extensos de lenguaje m√°s r√°pido y con menos recursos, sin p√©rdida de *accuracy* [^31]. En nuestro caso, intentamos primero usarlo para inferir. Como resultado, obtuvimos una velocidad de inferencia mayor comparada a la realizada sin el uso de esta herramienta, pero notamos una degradaci√≥n en la calidad de respuesta utilizando contextos largos (alrededor de 15.000 tokens), llegando incluso a generar res√∫menes en lugar de preguntas sobre el texto solicitado, por lo que optamos por no utilizarla.

Nuestra hip√≥tesis sobre esta degradaci√≥n se basa en la forma en la que Unsloth maneja los contextos a trav√©s de *RoPE Scaling* [^32]. Consideramos que, al procesar contextos de este tama√±o, el modelo podr√≠a perder la capacidad de retener la instrucci√≥n solicitada, tendiendo en su lugar, a realizar la instrucci√≥n m√°s com√∫n asociada con dicha longitud de contexto.

## Bases de c√≥digo exploradas

Dado a que era nuestra primera vez trabajando en un proyecto sobre el procesamiento de lenguaje natural, fue necesario explorar diferentes bases de c√≥digo para aprender las t√©cnicas m√°s utilizadas, las diferentes partes en el desarrollo y para mantener nuestro c√≥digo simple y f√°cil de leer pero modernizado, ya que varias de las librer√≠as utilizadas en el proyecto se actualizan de forma regular.



| Nombre                                                                                  | Repositorio                                                                                                      | Prop√≥sito                                                                          | Aspectos clave                                                                              | Motivo de an√°lisis                                                                                                                              |
| --------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------- |
|    ü§ó **Tokenizers - Hugging Face**                                                                                       |                                                                                             [huggingface/tokenizers](https://github.com/huggingface/tokenizers)                     | Pre-procesameinto de datos, inferencia y entrenamiento.                                                                                    |     Buena documentaci√≥n y muchos ejemplos.                                                                                        | Ofrece una implementaci√≥n optimizada para la tokenizaci√≥n r√°pida y eficiente, compatible con los modelos del Hub. |
| ü§ó **Datasets - Hugging Face**                                                          | [huggingface/datasets](https://github.com/huggingface/datasets)                                                  | Descarga y preprocesamiento de datos.                                              | Carga de conjunto de datos p√∫blicos de forma sencilla y pre-procesado eficiente.            | Ofrece c√≥digo para descargar conjuntos de datos y diferentes configuraciones para hacerlo.                                                      |
| ü§ó **TRL - Hugging Face**                                                               | [huggingface/trl](https://github.com/huggingface/trl)                                                            | *Fine tuning* usando aprendizaje supervisado.                                      | Soporte con PEFT, buena documentaci√≥n, eficiencia y `Trainers`.                             | Brinda implementaciones de entrenamientos, ejemplos de formatos de conjunto de datos utilizados para entrenar y ejemplos de configuraci√≥n de par√°metros de entrenamiento. |
| ü§ó **NLP Course - Hugging Face**                                                        | [huggingface/course](https://github.com/huggingface/course)                                                      | Inferencia de los distintos modelos probados y comprensi√≥n de conceptos claves.    | Aprendizaje guiado sobre el ecosistema de HuggingFace y conceptos clave de NLP.             | Instruirnos en el uso de `Transformers`, `Dataset` y `Tokenizers`.                                                                              |
| ü§ó **Transformers - Hugging Face**                                                      | [huggingface/transformers](https://github.com/huggingface/transformers)                                          | Implementaci√≥n de modelos LLM.                                                     | Compatibilidad con modelos Llama y comunidad activa.                                        | Base para hacer inferencia probando modelos mediante `pipeline`.                                                                                |
| üìñ **LLM Course**                                                                       | [mlabonne/llm-course](https://github.com/mlabonne/llm-course)                                                    | *Fine tuning* y pre-procesamiento de datos.                                        | Explicaciones detalladas, entrenamiento ultra-eficiente y c√≥digo f√°cil de leer.             | Aprender c√≥mo entrenar el modelo de manera eficiente, usando t√©cnicas de aprendizaje supervisado.                                               |
| ü§ó **PEFT - Hugging Face**                                                              | [huggingface/peft](https://github.com/huggingface/peft)                                                          | *Fine-tuning* usando QLoRA y valor de par√°metros para la aplicaci√≥n de la t√©cnica. | *Fine-tuning* eficiente y buena documentaci√≥n.                                              | Ofrece explicaci√≥n detallada acerca de los par√°metros para aplicar QLoRA y c√≥digo donde se aplica la t√©cnica.                                   |
| ‚ö° **Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA** | [colab/bnb-4bit-training](https://colab.research.google.com/drive/1VoYNfYDKcKRQRor98Zbf2-9VQTtGJ24k?usp=sharing) | *Fine tuning* de modelos mediante QLoRA y cuantizaci√≥n en NF4.                     | Uso del ecosistema de HuggingFace, *nested quantization*, NF4 y *Gradient checkpointing*.   | Ofrece una implementaci√≥n optimizada usando QLoRA y es compatible con HuggingFace.                                                              |
| ü¶• **Unsloth**                                                                          | [unslothai/unsloth](https://github.com/unslothai/unsloth)                                                        | Optimizaci√≥n para realizar inferencia y *fine tuning*.                             | Optimizaci√≥n de la memoria, compatibilidad con modelos Llama 3.1 y velocidad de inferencia. | Brinda modelos ya cuantizados, c√≥digo para entrenar y hacer inferencia de modelos de la familia Llama 3, usando la herramienta Unsloth en un entorno de HuggingFace.    |
| üìö **Wikipedia-API**                                                                    | [project/wikipedia-API](https://pypi.org/project/Wikipedia-API/)                                                 | Obtenci√≥n de datos de Wikipedia para probar modelos.                               | Simpleza y facilidad para recuperar informaci√≥n de Wikipedia.                               | Ofrece c√≥digo sobre el uso del *wrapper* mediante casos de uso.                                                                                 |

## Problemas encontrados

En esta secci√≥n detallamos los principales desaf√≠os que surgieron durante el desarrollo del proyecto y las estrategias implementadas para resolverlos.

### Recursos y entorno de ejecuci√≥n

Comenzamos utilizando Google Colab en el proyecto debido a ventajas como la accesibilidad, colaboraci√≥n en tiempo real y disponibilidad de GPU gratuitas. Continuamos usando esta plataforma en cada etapa, incluyendo el *fine tuning*. Sin embargo, a medida que el proyecto avanzaba, los recursos ofrecidos por este entorno comenzaron a limitarnos, lo que nos hizo recurrir a las t√©cnicas de cuantizaci√≥n y optimizaci√≥n del rendimiento mencionadas anteriormente.

### Elecci√≥n del tipo de preguntas a generar

En primera instancia, probamos el desempe√±o de diferentes modelos en la tarea de generar preguntas en base a un art√≠culo. No obstante, la comparaci√≥n entre las preguntas generadas cada vez fue m√°s dif√≠cil de realizar, lo que nos hizo pensar que nuestra forma de categorizar preguntas como "buenas" o "malas" carec√≠a de certeza, provocando incertidumbre al momento de decidir si la calidad de las preguntas generadas mejoraba. Para solucionarlo, definimos una serie de pautas acerca de la caracter√≠sticas que deseamos que las preguntas generadas por el modelo posean:
* *Tipos de preguntas:* los tipos de preguntas que se desean generar son de respuesta corta y sentencias a determinar en verdaderas o falsas. Adem√°s, se considerar√° que la respuestas a dichas preguntas est√©n presentes dentro del art√≠culo proporcionado.
    > **‚úÖ Ejemplos de pregunta de repuesta corta:**
    > 1. ¬øEn que a√±o naci√≥?
    > 2. ¬øCu√°l fue la fecha y causa de su muerte?
    > 3. ¬øD√≥nde estudi√≥?
    > 4. ¬øQu√© ocupaciones tuvo?
    > 
    > **‚úÖ Ejemplos de sentencias a determinar como verdaderas o falsas:**
    > 1. Bill Gates es el fundador de la empresa Apple. ¬øVerdadero o Falso?
    > 2. Bill Gates es un gran aficionado al tenis. ¬øVerdadero o Falso?
* *No repetici√≥n:* deseamos diversidad de preguntas, por lo tanto, dos de la misma lista no deben ser ni id√©nticas ni demasiado similares. 
    > ‚õî **Ejemplos de preguntas repetidas o similares a evitar:**
    > 1. ¬øEn qu√© a√±o naci√≥ Bill Gates?.
    > 2. ¬øQue edad tiene Bill Gates?.
* *Naturalidad y fluidez:* las preguntas generadas deben evitar una estructura mecanizada, minimizando repeticiones innecesarias y mostrando una formulaci√≥n de preguntas similiar a las realizadas por una persona.
    > ‚õî **Ejemplos de preguntas antinaturales (errores de formulaci√≥n):**
    >  1. ¬øQu√© campo de estudio se le considera uno de los m√°s importantes de todos los tiempos? (Correcci√≥n: ¬øEn qu√© --> ¬øQu√©).
    >  2.  ¬øQu√© universidad asisti√≥ G√∂del a estudiar despu√©s de cumplir 18 a√±os? (Corecci√≥n: ¬øA qu√© --> ¬øQu√©).
    >  3. ¬øQu√© fue el resultado de la investigaci√≥n policial sobre la muerte de Alan Turing en 2012? (Correcci√≥n: ¬øQu√© --> ¬øCu√°l).
    >  4. ¬øEn qu√© universidad asisti√≥ Steve Jobs? (Correcci√≥n: ¬øEn qu√© --> ¬øA qu√©)

    > ‚õî **Ejemplos de preguntas antinaturales (falta de naturalidad):**
    >  1. ¬øEn qu√© a√±o Bill Gates abandon√≥ la universidad para trasladarse a Albuquerque y pactar con MITS la cesi√≥n del 50% del lenguaje para computadoras Basic?  (posee una estructura mecanizada extra√≠da del texto directamente).
    >  2.  ¬øFue John von Neumann un matem√°tico h√∫ngaro-estadounidense que realiz√≥ contribuciones fundamentales en f√≠sica cu√°ntica, an√°lisis funcional, teor√≠a de conjuntos, teor√≠a de juegos, ciencias de la computaci√≥n, econom√≠a, an√°lisis num√©rico, cibern√©tica, hidrodin√°mica, estad√≠stica y muchos otros campos? (al igual que el primer ejemplo, una pregunta demasiado espec√≠fica).
    >  3. ¬øVerdadero o falso: La reina Isabel II le otorg√≥ a Alan Turing un indulto p√≥stumo en 2013? (no es una forma natural de preguntar acerca de verdaderos o falsos).

    > ‚õî **Ejemplos de preguntas antinaturales (estructura mecanizada):**
    >  1. ¬ø**Fue** el gobierno brit√°nico de David Cameron el que deneg√≥ el indulto a Alan Turing en 2012?
    >  2. ¬ø**Fue** el premio Turing otorgado anualmente por la Association for Computing Machinery para personas destacadas por sus contribuciones t√©cnicas al mundo de la inform√°tica?
    > 3. ¬ø**Fue** el Instituto Alan Turing inaugurado por el UMIST y la Universidad de M√°nchester en el verano de 2004?
    > 4. ¬ø**Fue** la estatua de bronce de Alan Turing esculpida por John W. Mills en la Universidad de Surrey en 2004?
* *Completitud:* priorizaremos la relevancia de las preguntas pero tendremos en cuenta que sean abarcativas a los temas tratados en el art√≠culo.
* *Orientadas al refuerzo de la comprensi√≥n:* las preguntas buscar√°n evaluar a quien las responda en su habilidad para identificar ideas principales y detalles espec√≠ficos (por ejemplo: fechas).
    > **‚úÖ Ejemplos de pregunta que refuerzan la comprensi√≥n:**
    > 1. ¬øCu√°l fue la fecha de nacimiento de John von Neumann?
    > 2. Cillian Murphy gan√≥ el √ìscar como mejor actor por su papel en la pel√≠cula Oppenheimer, ¬øVerdadero o Falso? (es una informaci√≥n importante para la biograf√≠a de un actor)

### Elecci√≥n del modelo

Definir el modelo y la versi√≥n a usar del mismo nos llevo m√°s tiempo del que esper√°bamos, ya que tuvimos que investigar y probar diferentes posibilidades.

Deb√≠amos buscar modelos que tengan soporte para la generaci√≥n de texto en espa√±ol. En dicha b√∫squeda, encontramos a la familia de modelos LLaMa en su versi√≥n 3.1 que son multiling√ºes [^33]. Para comprender la importancia de este punto, en la siguiente subsecci√≥n mostraremos una comparativa sencilla entre la capacidad de generaci√≥n de texto en espa√±ol del modelo `meta-llama/Llama-2-13b-hf`, que no tiene soporte oficial para el idioma, y `meta-llama/Llama-3.1-8B`.

Adem√°s, como mencionamos en la secci√≥n *system prompt*, terminamos eligiendo el modelo `meta-llama/Llama-3.1-8B-Instruct` ya que nos otorgaba m√°s facilidades para solicitarle instrucciones.

#### Comparativa: soporte para la generaci√≥n de texto en espa√±ol

La comparativa const√≥ en solicitarle a ambos modelos completar una sentencia que comenzaba con `"El perro estaba"`, con los mismos par√°metros de generaci√≥n, entre los cuales incluimos un n√∫mero para `max_new_tokens` y as√≠ evitar secuencias generadas que sean muy largas. La siguiente imagen es un ejemplo de los textos generados por ambos modelos.

![Compartiva](https://i.imgur.com/UsBXOZ6.png)

Como ya hemos mencionado a lo largo del art√≠culo, la elecci√≥n del modelo  `"meta-llama/Llama-3.1-8B-Instruct"` fue clave para el *prompting*, debido a que puede interpretar instrucciones y usar roles en su entrada para adaptar su salida [^34].

### Reducir el dominio

Los art√≠culos presentes en Wikipedia pueden tratarse de diversos temas, asimismo una pregunta puede ser menos relevante en un art√≠culo que en otro. Es por eso que decidimos reducir el dominio de los art√≠culos a solo biograf√≠as, sobre los cuales evaluaremos las preguntas generadas. De esta manera, ser√≠a m√°s f√°cil evaluar la calidad de las preguntas y detectar las limitaciones del modelo.

### Respuestas no solicitadas junto con las preguntas generadas

La tarea que le solicitamos al modelo siempre fue generar preguntas a partir de un determinado contexto. Uno de los problemas que surgieron con esta solicitud fue que el modelo tambi√©n generaba las respuestas de dichas preguntas, inclusive si le especific√°bamos expl√≠citamente que no lo hiciera.

#### Ejemplo de respuestas no solicitadas junto a las preguntas generadas
A continuaci√≥n, mostramos un ejemplo de salida junto a su entrada que nos sucedi√≥ usando el modelo instruido.

``` Python
# Entrada suministrada al modelo
prompt = [
    {
        "role": "system",
        "content": (
            "Eres un generador de preguntas de lectura comprensiva. "
            "Tu tarea es generar preguntas, sin incluir las repsuestas " 
            "y ademas algunas de las generadas deben ser de tipo "
            "verdadero o falso"
        ),
    },
    {
        "role": "user",
        "content": (
            "Genera exactamente 10 preguntas, de las cuales 5 deben ser de tipo "
            "verdadero o falso, sin incluir las respuestas, sobre el siguiente texto:\n\n"
            f"{article}"
        ),
    },
```


> **Salida producida por el modelo `meta-llama/Llama-3.1-8B-Instruct`:**
> 1. ¬øCu√°l fue el a√±o en que Steve Jobs fund√≥ Apple?
> 2. ¬øCu√°l fue el nombre del primer ordenador personal de Apple que usaba una interfaz gr√°fica de usuario (GUI) y un rat√≥n?
> 3. ¬øVerdad o falsedad: Steve Jobs se convirti√≥ en el mayor accionista individual de Disney despu√©s de la fusi√≥n de Pixar con la empresa.
> 4. ¬øCu√°l fue el nombre del software de desarrollo que se bas√≥ en el sistema operativo NeXTSTEP?
> 5. ¬øVerdad o falsedad: Steve Jobs fue elegido como el mejor emprendedor de la historia moderna por la revista Fortune en 2012.
> 
> Respuestas:
> 1. 1976
> 2. Macintosh 128K
> 3. Verdadero
> 4. NeXTSTEP
> 5. Verdadero

A este problema pudimos solucionarlo parcialmente, mediante la t√©cnica de *few shot prompting* en conjunto con una mejora en la redacci√≥n de la solicitud suministrada, realizando una m√°s corta y directa. 

#### Ejemplo donde aplicamos la soluci√≥n
El siguiente fragmento de c√≥digo muestra una de nuestras entradas con las mejoras mencionadas.
```python
# Ejemplo de preguntas para el art√≠culo de Turing
turing_question_examples = """
Preguntas de tipo respuestas cortas:
1. ¬øEn qu√© fecha naci√≥ Alan Turing?
2. ¬øD√≥nde naci√≥ Alan Turing?
3. ¬øCu√°les eran las profesiones de Alan Turing?
4. ¬øCu√°l fue la causa de su muerte?
5. ¬øA qu√© se dedic√≥ durante la Segunda Guerra Mundial?

Preguntas de tipo Verdadero o Falso:
6. La m√°quina sobre la cual trabaj√≥ durante la guerra se llam√≥ m√°quina Enigma. ¬øVerdadero o Falso?
7. Se estima que el trabajo de Turing durante la guerra acort√≥ la misma entre 2 y 4 a√±os. ¬øVerdadero o Falso?
8. La prueba de Turing se utiliza para juzgar la computabilidad de una funci√≥n. ¬øVerdadero o Falso?
9. Alan Turing fue condenado por cr√≠menes de guerra despu√©s de la Segunda Guerra Mundial. ¬øVerdadero o Falso?
"""

# Ejemplo de preguntas para el art√≠culo de Von Neumann
von_neumann_question_examples = """
Preguntas de tipo respuestas cortas:
1. ¬øA qu√© se dedicaba John von Neumann?
2. ¬øCu√°l fue la fecha de su nacimiento?
3. ¬øEn qu√© universidad obtuvo su doctorado en matem√°ticas?
4. ¬øCu√°l fue su aporte en la Segunda Guerra Mundial?

Preguntas de tipo Verdadero o Falso:
5. Es uno de los matem√°ticos m√°s importantes del siglo XX. ¬øVerdadero o Falso?
6. Asisti√≥ a la Universidad de Budapest. ¬øVerdadero o Falso?
7. John von Neumann particip√≥ en el famoso Proyecto Manhattan. ¬øVerdadero o Falso?
"""

# Entrada del modelo
prompt = [
    {
        "role": "system",
        "content": (
            "Eres un generador de preguntas de lectura comprensiva. "
            "Tu tarea es crear preguntas basadas en un texto dado, "
            "sin incluir respuestas ni opciones. Solo debes generar preguntas, "
            "incluyendo preguntas de tipo verdadero o falso."
        ),
    },
    {
        "role": "user",
        "content": (
            f"Usando el siguiente art√≠culo {turing}, estos son ejemplos de preguntas que puedes hacer:\n"
            f"{turing_question_examples}\n\n"
            f"Usando el siguiente art√≠culo {von_neumann}, estos son ejemplos de preguntas que puedes hacer:\n"
            f"{von_neumann_question_examples}\n\n"
            f"Genera exactamente 10 preguntas, incluyendo 5 que deben ser de tipo verdadero o falso, sobre el siguiente texto:\n{article}"
        ),
    },
]
```

Sin embargo, la soluci√≥n definitiva a este problema fue realizar *fine tuning*.

### Error en una URL del conjunto de datos

El conjunto de datos que decidimos utilizar est√° presente como `Dataset` en Hugging Face [^11], para facilitar su uso mediante su API. No obstante, el m√≥dulo encargado de descargarlo llamado `SQAC.py` posee un error en una URL que nos lleva a una p√°gina que no existe, siendo imposible de obtenerlo mediante el comando `load_dataset("PlanTL-GOB-ES/SQAC")`. Para solucionarlo, debimos descargar los archivos del conjunto de datos desde la p√°gina de HF y cambiar el valor de la variable `_URL` del archivo `SQAC.py` por `"https://huggingface.co/datasets/PlanTL-GOB-ES/SQAC/resolve/main/"`.

Este error lleva tiempo ah√≠, inclusive un usuario solicit√≥ un *pull request* con la soluci√≥n, pero a√∫n no ha sido aceptada [^35].

### Familia de modelos LLama3 y los tokens de relleno

El tokenizador de la familia de modelos LLaMa no posee un `pad_token` definido, lo que es un problema para hacer inferencia, y tambi√©n, para aplicar *fine tuning* sobre ellos. Hemos investigado mucho al respecto y es un hecho, la falta de informaci√≥n oficial sobre este tema provoca una gran confusi√≥n entre los usuarios [^36][^37]. La soluci√≥n que abordamos para este problema fue la que habitualmente se utiliza, definir el `pad_token` igual que el `eos_token` y mantener un tama√±o de *batch* de 1 durante el entrenamiento para evitar problemas debido al token de relleno.

### Formato del conjunto de datos

Adoptamos como primer enfoque implementar la t√©cnica de *fine tuning* sobre 1000 preguntas repartidas en 840 contextos diferentes del conjunto de datos SQAC. No obstante, en el entrenamiento del modelo pudimos notar que algo andaba mal ya que el *training loss* no disminu√≠a, sino que oscilaba siempre entre los mismos valores, como se puede observar en el siguiente gr√°fico.

![Training loss sin cambios](https://i.imgur.com/wPDLTNc.pngg)

Al finalizar el entrenamiento y evaluar el modelo, observamos que la generaci√≥n de texto carec√≠a de coherencia y naturalidad, ya que el modelo tend√≠a a repetir las pocas preguntas que lograba formular y no inclu√≠a un token especial de terminaci√≥n, sino que entraba en un bucle generando contenido repetitivo (lo que suele ser un problema entre los usuarios [^38]).

> **‚õî Ejemplo de salida modelo entrenado con 1000 preguntas:**
> Aqu√≠ tienes algunas preguntas sobre el texto proporcionado 
>  1. ¬øCu√°l fue el papel de Steve Jobs en el desarrollo del ordenador personal?
> 2. ¬øC√≥mo fue recibido el lanzamiento del iPod?
> 3. ¬øCu√°ndo obtuvo Steve Jobs la condici√≥n de millonario?
> 4. ¬øCu√°l fue el papel de Steve Jobs en el √©xito de Pixar?
> 5. ¬øCu√°ndo falleci√≥ Steve Jobs? 
> 
> Cutting Knowledge Date: December 2023
> Today Date: 26 Jul 2024
> 
> Eres un generador de preguntas de lectura comprensiva. Tu tarea es crear preguntas basadas en un texto dado, sin incluir respuestas ni opciones. Solo debes generar preguntas, incluyendo preguntas de tipo verdadero o falso.<|end_header_id|><|start_header_id|>user<|end_header_id|>
> 
> Genera exactamente 3 preguntas, sobre el siguiente texto:
> Biograf√≠a
> Primeros a√±os
> Steven Jobs naci√≥ en San Francisco (California) en 1955,[25]‚Äã[26]‚Äã[27]‚Äã fruto de la relaci√≥n entre Abdulfattah Jandali, un refugiado sirio musulm√°n y Joanne Carole Schieble, una estadounidense de ascendencia alemana y suiza, por entonces...

Nuestra hip√≥tesis fue que el modelo no lograba captar el patr√≥n en los datos suministrados para lograr aprender de ellos. En una primera instancia, pensamos que era porque hab√≠a demasiados tipos de art√≠culos distintos en los fragmentos del conjunto de datos, por lo que probamos entrenar el modelo nuevamente. Esta vez, filtrando aquellos contextos que provengan de la secci√≥n "Biograf√≠a" de alg√∫n art√≠culo y entrenando el modelo con conversaciones formuladas a partir de las preguntas asociadas a tales contextos.

Cabe aclarar que un mismo contexto puede poseer varias preguntas asociadas, lo que provoca que se repita en el conjunto de datos (tantas veces como preguntas asociadas tenga), entonces fue necesario agrupar las preguntas por contexto para generar las conversaciones antes mencionadas.

![Comparacion training loss 1](https://i.imgur.com/w6N63VM.png)

Como podemos ver en la imagen anterior, este nuevo enfoque hizo que el *training loss* disminuyera. Sin embargo, los problemas de generaci√≥n segu√≠an sin ser solucionados. Por lo tanto, la causa del problema era otra y, efectivamente, revisando la documentaci√≥n de las funciones que hab√≠amos usado para entrenar el modelo, notamos que `SFTTrainer` ten√≠a un formato espec√≠fico para recibir el conjunto de datos de entrenamiento sin necesidad de ser pre-procesado directamente antes, que es lo est√°bamos haciendo mediante el `tokenizer` del modelo [^39]. Entonces, elegimos el formato conversacional, que posee la siguiente estructura:

```python
{"messages": [{"role": "system", "content": "You are helpful"}, {"role": "user", "content": "What's the capital of France?"}, {"role": "assistant", "content": "..."}]}
{"messages": [{"role": "system", "content": "You are helpful"}, {"role": "user", "content": "Who wrote 'Romeo and Juliet'?"}, {"role": "assistant", "content": "..."}]}
```
Con este, como podemos ver en la imagen a continuaci√≥n obtuvimos una mayor disminuci√≥n del *training loss* y adem√°s, el modelo entrenado genera un token especial para indicar cuando ha terminado.

![Comparacion training loss 2](https://i.imgur.com/ajH9UgV.png)
> **‚úÖ Ejemplo de salida modelo entrenado con biograf√≠as y formateo que genera token de terminaci√≥n:**
> 1. ¬øEn qu√© a√±o naci√≥ Bill Gates? 
> 2. ¬øCu√°l es la ocupaci√≥n actual de Bill Gates en Microsoft despu√©s de dejar el cargo de CEO en 2008?
> 3. ¬øCu√°l fue la ocupaci√≥n de Bill Gates en Microsoft durante su carrera en la empresa?
> 4. ¬øCon qui√©n se cas√≥ Bill Gates en 1994?
> 5. ¬øCu√°l es la fortuna estimada de Bill Gates seg√∫n la revista Forbes en 2023?
> 6. ¬øCon qui√©n firm√≥ un acuerdo de donaciones en 1999?
> 7. ¬øCu√°l es la organizaci√≥n con la que Bill Gates y su esposa, Melinda, trabajan juntos?
> 8. ¬øCu√°ndo dej√≥ Bill Gates de dirigir Microsoft?
> 9. ¬øCu√°l es la segunda ocupaci√≥n de Bill Gates desde 2007?
> 10. ¬øCu√°l es el honor que recibi√≥ Bill Gates en 2005 de parte del Reino Unido?**<|eot_id|>**
> 
> (**<|eot_id|>**: token que genera el modelo para indicar el fin de su turno)


###  Evaluaci√≥n del progreso
Durante el proyecto realizamos una evaluaci√≥n manual anecd√≥tica, analizando los resultados obtenidos al variar las entradas y las t√©cnicas empleadas. Inicialmente, pensamos en aplicar *vector similarity measurament*, que comparan el vector generado con un vector objetivo para evaluar su similitud [^40]. Sin embargo, consideramos que esta m√©trica autom√°tica de evaluaci√≥n no era capaz de capturar percepciones humanas, como la calidad y la relevancia de las preguntas generadas [^41], por lo cual fue descartada.

En el futuro, planeamos investigar y desarrollar m√©tricas de evaluaci√≥n m√°s adecuadas que se compongan de an√°lisis autom√°ticos y evaluaciones humanas, para evaluar no solo la similitud sem√°tica, sino tambi√©n la coherencia, naturalidad y utilidad de las preguntas.

## Conclusiones
En esta secci√≥n abordaremos diferentes conclusiones que pudimos obtener de realizar este proyecto.

### Planificaci√≥n inicial y ejecuci√≥n efectiva

Nuestra falta de experiencia en el procesamiento de lenguaje natural mediante modelos extensos de lenguaje caus√≥ la omisi√≥n involuntaria de aspectos importantes durante la etapa de planificaci√≥n del proyecto, como la elecci√≥n de un modelo apropiado, criterios de evaluaci√≥n para los resultados obtenidos y hasta falta de precisi√≥n en los objetivos espec√≠ficos. Todos estos detalles, obstaculizaron el progreso y nos obligaron a regresar a la etapa de planificaci√≥n en diversas ocasiones a subsanar los errores cometidos.

La impericia mencionada en el p√°rrafo anterior ocasion√≥ tambi√©n una mala gesti√≥n del tiempo, donde tareas que pens√°bamos terminar r√°pidamente nos llevaron mucho m√°s tiempo de lo estipulado. De esta manera, terminamos aplicando cada t√©cnica planteada, con el fin de cumplir con todos los objetivos en el tiempo acordado, dejando en segundo plano la evaluaci√≥n de las mejoras obtenidas, lo que produjo una falta de comprensi√≥n en los impactos de cada una.

### Entorno de ejecuci√≥n limitado

Mencionamos en reiteradas ocasiones a lo largo de este informe que el entorno de ejecuci√≥n sobre el cual est√°bamos trabajando pose√≠a recursos limitados, forz√°ndonos a tomar ciertas decisiones para seguir trabajando sobre √©l. Realmente, trabajar sobre este entorno fue una elecci√≥n, ya que no era el √∫nico que dispon√≠amos, sino que tambi√©n cont√°bamos con acceso a una m√°quina del Centro de Computaci√≥n de Alto Desempe√±o de la UNC (CCAD), con mejores especificaciones.

Consideramos que esta decisi√≥n produjo invertir demasiado tiempo en investigar t√©cnicas de optimizaci√≥n avanzadas, el cual nos hubi√©semos ahorrado eligiendo el otro entorno. Sin embargo, debemos mencionar que trabajar en un entorno limitado nos hizo descubrir que este es un problema usual en esta √°rea y nos dio a conocer t√©cnicas que la comunidad emplea para solucionarlo.

### Conclusiones sobre t√©cnicas utilizadas

Mediante la t√©cnica *zero-shot* aplicada al modelo base `meta-llama/Llama-3.1-8B` obtuvimos preguntas que, en su mayor√≠a, ni siquiera cumpl√≠an con el  tipo especificado, incluso generaba preguntas repetidas, muy similares entre s√≠ y hasta inclu√≠a respuestas. Adem√°s, era complicado indicarle al modelo el n√∫mero de preguntas a generar, por lo que a veces generaba m√°s o menos de las solicitadas, dependiendo del par√°metro de detenci√≥n que le pas√°bamos. 

> **üìç Ejemplo de salida `Llama-3.1-8B` usando zero-shot:**
>En base al texto anterior, responde las siguientes 10 preguntas:
> 1. ¬øCu√°l es el nombre del cofundador de Apple?
> 2. ¬øCu√°l es la empresa de la que Jobs era el mayor accionista?
> 3. ¬øCu√°l fue el nombre del primer ordenador personal de Apple con rat√≥n?
> 4. ¬øCu√°l es la empresa subsidiaria de Lucasfilm que Jobs transform√≥ en Pixar?
> 5. ¬øCu√°l fue el nombre de la empresa de la que Jobs se fue en 1985?
> 6. ¬øCu√°l fue la primera estaci√≥n de trabajo de NeXT?
> 7. ¬øCu√°l fue la empresa que Jobs fund√≥ en 1993?
> 8. ¬øCu√°l es la empresa que Jobs adquiri√≥ en 1997?
> 9. ¬øCu√°l fue el nombre del primer computador personal con Interfaz gr√°fica del usuario?
> 10. ¬øCu√°l fue el nombre de la empresa de la que Jobs se fue en 1985?
> 
>  References
> 
> 1.  ¬´Steve Jobs, el hombre que cambi√≥ el mundo¬ª. El Mundo. 6 de octubre de 2011. Consultado el 17 de octubre de 2011.
> 2.  ¬´Steve Jobs, el hombre que cambi√≥ el mundo¬ª. El Mundo. 6 de octubre de 2011. Consultado el 17 de oct....
>
>

Sobre este mismo modelo, aplicamos *few-shot* con el fin de obtener preguntas que se adecuaran a los tipos especificados y lo conseguimos, pero segu√≠amos con las mismas deficiencias que la t√©cnica anterior, entonces decidimos cambiar el modelo.

Aplicando *system prompts* y *few shot* al modelo `meta-llama/Llama-3.1-8B` conseguimos mejorar notablemente la calidad de las preguntas generadas y era posible especificar, sin problemas, la cantidad a generar, inclusive pod√≠amos pedir cu√°ntas preguntas de cada tipo quer√≠amos. Este modelo normalmente generaba preguntas dentro de los tipos detallados, aunque sol√≠a generarlas con respuestas una estructura mecanizada o no orientadas al refuerzo de la comprensi√≥n. 

> **üìç Ejemplo de salida `Llama-3.1-8B-Instruct` usando few-shot solicitando 5 preguntas de repuesta corta y 5 de sentencias a determinar como verdaderas o falsas:**
> 
> Preguntas de tipo respuestas cortas:
> 1. ¬øEn qu√© fecha naci√≥ Steve Jobs?
> 2. ¬øD√≥nde naci√≥ Steve Jobs?
> 3. ¬øCu√°l fue el nombre de la empresa que fund√≥ Steve Jobs?
> 4. ¬øCu√°l fue el nombre del primer computador personal de Apple?
> 5. ¬øCu√°l fue el nombre de la tienda de m√∫sica en l√≠nea de Apple?
> 
> Preguntas de tipo Verdadero o Falso:
> 
> 6. Steve Jobs fue el fundador de Apple. ¬øVerdadero o Falso?
> 7. El Macintosh 128K fue el primer ordenador personal que usaba una interfaz gr√°fica de usuario (GUI). ¬øVerdadero o Falso?
> 8. Steve Jobs fue el director ejecutivo de Apple hasta su muerte. ¬øVerdadero o Falso?
> 9. El iPad 2 fue lanzado en 2010. ¬øVerdadero o Falso?
> 10. Steve Jobs recibi√≥ un trasplante de h√≠gado en 2009. ¬øVerdadero o Falso?

*Few-shot*, en este caso, lo aplicamos pasando otros art√≠culos junto con preguntas de ejemplo que se pod√≠an realizar sobre los mismos, antes de la instrucci√≥n, en un formato de conversaci√≥n. 

Finalmente con *fine tuning* solucionamos el problema de preguntas que inclu√≠an respuestas, aunque invertimos bastante tiempo en elegir el conjunto de datos apropiado, pre-procesarlo y entrenar el modelo sobre √©l. El modelo resultante genera preguntas que cumplen con el tipo de respuesta corta, poseen  naturalidad, fluidez y cuya respuesta est√° presente en el texto proporcionado, todo esto debido a la naturaleza del conjunto de datos.

> **üìç Ejemplo de salida del modelo resultante de aplicar la t√©cnica de fine tuning `Llama-3.1-8B-Instruct`**
>
> ##### Ejemplo 1: solicitando 10 preguntas de repuesta corta, incluyendo 5 sentencias a determinar como verdaderas o falsas:
> 1. ¬øEn qu√© ciudad naci√≥ Bill Gates? 
> 2. Verdadero o falso: Bill Gates es hijo de dos padres extranjeros.
> 3. ¬øCon qui√©n fund√≥ Microsoft? 
> 4. ¬øCu√°l fue el primer ordenador que utiliz√≥ Bill Gates en la escuela? 
> 5. Verdadero o falo: Bill Gates abandon√≥ Harvard en 1975.
> 6. ¬øA qu√© se dedic√≥ Bill Gates despu√©s de abandonar sus labores diarias en frente de Microsoft en 2008? 
> 7. ¬øCu√°l fue el primer sistema operativo que vendi√≥ Bill Gates a IBM en 1981? 
> 8. ¬øCu√°l es la fortuna de Bill Gates seg√∫n la revista Forbes en 2023? 
> 9. Verdadero o falo: Bill Gates ha sido galardonado con el Premio Pr√≠ncipe de Asturias en varias ocasiones.
> 10. ¬øCu√°nte tiempo lleva Bill Gates siendo uno de los hombres m√°s ricos del mundo seg√∫n la revista Forbes?
> ##### Ejemplo 2: solicitando 10 preguntas y que incluya verdaderos o falsos 
>  1. ¬øEn qu√© a√±o naci√≥ Bill Gates? 
> 2. ¬øCu√°l es el cargo que Bill Gates ocup√≥ en Microsoft durante su carrera en la empresa?
> 3. ¬øCon qui√©n fund√≥ Gates la empresa Microsoft?
> 4. ¬øCu√°l es la fortuna de Bill Gates seg√∫n la revista Forbes en el a√±o 2023?
> 5. ¬øC√≥mo se llama el fruto del trabajo de Bill Gates y Warren Buffett?
> 6. ¬øEn qu√© a√±o abandon√≥ Gates sus labores diarias al frente de Microsoft?
> 7. ¬øCu√°l es el nombre de la universidad a la que asisti√≥ Gates antes de abandonarla en 1981?
> 8. ¬øCu√°l es el nombre del proyecto al que se comprometieron Bill Gates y Warren Buffett en 2009?
> 9. ¬øEs cierto o falso que Bill Gates no ha cambiado su estilo de vida desde que dej√≥ de dirigir Microsoft en 2008?
> 10. ¬øA qui√©n otorg√≥ la revista EE. UU. la distinci√≥n de ¬´50 solteros m√°s codiciados¬ª en 1986?

Como conclusi√≥n de esta secci√≥n consideramos que para esta tarea, realizar *few-shot* con un *prompt* bien estructurado es m√°s que suficiente. El modelo obtenido mediante *fine tuning* conlleva mucho esfuerzo pero sigue teniendo limitaciones importantes, sobre todo con las preguntas de tipo verdadero o falso, al no tener ejemplo de ellas en su conjunto de datos.

### Trabajo Futuro

Si bien logramos mejorar la generaci√≥n de preguntas mediante *fine tuning* y *few shot prompting*, quedan desaf√≠os que abordar, como la diversificaci√≥n de resultados, ya que recordemos que lo trabajado en este proyecto fue enfocado solamente a biograf√≠as y nos gustar√≠a poder ampliar este trabajo a otros tipos de texto. Asimismo, nos gustar√≠a mejorar la evaluaci√≥n sobre las preguntas generadas y la exploraci√≥n de otras limitaciones.

****************************************



### Referencias
[^1]: https://urjconline.atavist.com/2023/06/29/pregunta-tipo-respuesta-corta/.
[^2]: https://www.promptingguide.ai/techniques/zeroshot.
[^3]: https://promptengineering.org/system-prompts-in-large-language-models.
[^4]: https://www.promptingguide.ai/techniques/fewshot.
[^5]: https://huggingface.co/blog/llama31#how-to-prompt-llama-31.
[^6]: https://www.llama.com/docs/how-to-guides/quantization/.
[^7]: https://www.datacamp.com/tutorial/quantization-for-large-language-models.
[^8]:  https://llamaimodel.com/requirements/.
[^9]: https://huggingface.co/blog/4bit-transformers-bitsandbytes.
[^10]: https://toloka.ai/blog/base-llm-vs-instruction-tuned-llm/.
[^11]: https://www.turing.com/resources/finetuning-large-language-models#primary-fine-tuning-approaches
[^12]: https://huggingface.co/datasets/PlanTL-GOB-ES/SQAC.
[^13]: https://portal.odesia.uned.es/dataset/sqac-es.
[^14]: https://huggingface.co/docs/transformers/v4.49.0/es/index.
[^15]: https://about.fb.com/ltam/news/2024/07/presentamos-llama-3-1-nuestro-modelo-de-lenguaje-a-gran-escala-mas-capaz-hasta-la-fecha/.
[^16]: https://huggingface.co/docs.
[^17]: https://huggingface.co/docs/transformers/philosophy.
[^18]: https://pytorch.org/docs/stable/index.html.
[^19]: https://www.ibm.com/es-es/topics/pytorch.
[^20]: https://huggingface.co/docs/transformers/index#supported-models-and-frameworks.
[^21]: https://huggingface.co/docs/bitsandbytes/index.
[^22]: https://wikipedia-api.readthedocs.io/en/latest/?badge=latest.
[^23]: https://wikipedia.readthedocs.io/en/latest/code.html#module-wikipedia.exceptions.
[^24]: https://huggingface.co/docs/datasets/index.
[^25]: https://huggingface.co/learn/nlp-course/en/chapter5/4.
[^26]: https://pandas.pydata.org/.
[^27]: https://huggingface.co/docs/trl/index.
[^28]: https://en.wikipedia.org/wiki/Supervised_learning.
[^29]: https://huggingface.co/docs/peft/index.
[^30]: https://docs.wandb.ai/guides/#what-is-wb.
[^31]: https://unsloth.ai/introducing.
[^32]: https://huggingface.co/blog/unsloth-trl.
[^33]: https://ai.meta.com/blog/meta-llama-3-1/.
[^34]: https://www.llama.com/docs/how-to-guides/prompting.
[^35]: https://huggingface.co/datasets/PlanTL-GOB-ES/SQAC/discussions/4.
[^36]: https://discuss.huggingface.co/t/llama-pad-token/48001.
[^37]: https://discuss.huggingface.co/t/how-to-set-the-pad-token-for-meta-llama-llama-3-models/103418/6.
[^38]: https://huggingface.co/meta-llama/Meta-Llama-3-8B/discussions/60.
[^39]: https://huggingface.co/docs/trl/v0.11.1/en/sft_trainer#dataset-format-support.
[^40]: https://medium.com/advanced-deep-learning/understanding-vector-similarity-b9c10f7506de
[^41]: https://inteligenciaartificial360.com/fundamentos-ia/modelos-de-lenguaje/evaluacion-y-metricas-de-rendimiento-en-modelos-de-lenguaje/.
* Medina, Julia A. (2024). *Exploraci√≥n de t√©cnicas de prompt-programming para sistemas de recomendaci√≥n*. Universidad Nacional de C√≥rdoba.
