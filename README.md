# Generaci√≥n de preguntas de lectura comprensiva en espa√±ol utilizando LLM

#### Resumen
Este proyecto presenta una prueba de concepto para un generador de preguntas de lectura comprensiva en espa√±ol, que se enfocar√° principalmente en dos tipos de preguntas: de respuesta corta [^1] y afirmaciones a evaluar como verdaderas o falsas. Para ello, utilizaremos el modelo de lenguaje `"Llama-3.1-8B-Instruct"` aplicando distintas t√©cnicas y evaluando sus posibles mejor√≠as en el desempe√±o. A lo largo de este proyecto trabajaremos principalmente sobre art√≠culos de Wikipedia. Consideramos este estudio de inter√©s dado a que la viabilidad de un sistema con estas caracter√≠sticas podr√≠a reducir la carga horaria de profesores y reforzar el aprendizaje de los estudiantes sobre los textos.

**Autores:** *Franco Artico* y *Nain Cadro*.

## Hip√≥tesis, objetivos iniciales y el estado final alcanzado

Nuestra **hip√≥tesis inicial** planteaba que el modelo seleccionado presentar√≠a dificultades en la generaci√≥n de preguntas de lectura comprensiva a partir de un texto en espa√±ol, mostrando errores sint√°cticos o generando preguntas desalineadas a las estipuladas en el proyecto. Para poner a prueba nuestra hip√≥tesis, primero investigamos y seleccionamos un modelo para evaluarlo. Adem√°s, fue imprescindible definir el tipo de preguntas que dese√°bamos generar y ciertos criterios para evaluarlas.

Con el modelo seleccionado (`meta-llama/Meta-Llama-3-8B-Instruct`) realizamos diversas pruebas las cuales mostraron que, efectivamente, presentaba deficiencias en la tarea solicitada las cuales pudieron ser identificadas y abordadas mediante las t√©cnicas establecidas en los objetivos.

Durante el desarrollo del proyecto, ajustamos nuestro enfoque, reduciendo el dominio del problema a biograf√≠as o fragmentos de ellas. Este cambio nos permiti√≥ identificar dificultades m√°s f√°cilmente en la generaci√≥n de preguntas en comparaci√≥n con trabajar con la extensa variedad de art√≠culos disponibles en Wikipedia.

De esta manera, la siguiente lista es c√≥mo quedaron constituidos los objetivos que nos propusimos alcanzar a lo largo del proyecto:
1. Investigar acerca de modelos de lenguaje entrenados para generar texto en espa√±ol.
2. Seleccionar el tipo de texto espec√≠fico sobre el cual el modelo realizar√° preguntas.
3. Definir cr√≠terios de evaluaci√≥n para las preguntas generadas por el modelo de acuerdo con el objetivo del proyecto.
4. Detectar limitaciones del modelo elegido para la generaci√≥n de preguntas en espa√±ol.
5. Aplicar *zero-shot prompting* y evaluar los resultados obtenidos.
6. Aplicar *few shot prompting* y evaluar los resultados obtenidos.
7. Elegir un conjunto de datos apropiado para realizar el *fine tuning* del modelo.
8. Efectuar *fine tuning* sobre el modelo y buscar mejores resultados respecto al punto anterior.
9. Determinar las mejoras obtenidas en cada enfoque y realizar conclusiones acerca de las mismas.

A trav√©s de estos objetivos, logramos abordar las limitaciones detectadas del modelo y aplicar diferentes t√©cnicas para mejorar la calidad de las preguntas generadas. Los resultados obtenidos mediante estas t√©cnicas ser√°n analizados con m√°s detalle en las conclusiones del informe. 

## T√©cnicas relevantes

Existen diferentes t√©cnicas para lograr que un modelo realice de forma correcta una tarea espec√≠fica. En este proyecto se abordaron las siguientes t√©cnicas: *zero-shot learning*, *few-shot learning*, *system prompts*, cuantizaci√≥n y *fine tuning*.

### Zero-shot learning
*Zero-shot learning* (sin ejemplos) es una t√©cnica que consiste en interactuar con el modelo proporcionando la instrucci√≥n a realizar sin incluir ejemplos ni demostraciones de c√≥mo queremos que sea realizada [^2].

Esta t√©cnica la utilizamos inicialmente buscando descubrir la clase de preguntas generadas por los modelos base de la familia Llama 3.1, los cuales no poseen un entrenamiento espec√≠fico en esta tarea.

Un modelo base no sigue instrucciones. Fue entrenado sobre una gran cantidad de datos para adquirir conocimientos b√°sicos acerca del lenguaje, conocimiento general y contextos, por lo que es √∫til en situaciones donde debe dar respuestas de conocimiento general o conversaciones casuales. Sin embargo, para obtener lo que deseamos en este proyecto debemos solicitarlo de forma diferente, agregando indicaciones extras o gu√≠as adicionales [^3].

A continuaci√≥n, presentamos un ejemplo de entrada que usamos para probar el modelo `meta-llama/Llama-3.1-8B`:

```python
prompt = article[0] + article[1] + "\n\n En base al texto anterior, responder las siguientes 5 preguntas:"
```

siendo `article[0]` y `article[1]` el primer y segundo p√°rrafo respectivamente de un art√≠culo de Wikipedia. De esta manera, incentivamos al modelo a generar preguntas respecto al texto que le proporcionamos.

### Few-shot learning

*Few-shot learining* es una t√©cnica que consiste en pasarle al modelo, junto con la instrucci√≥n de la tarea, unos pocos ejemplos que sirvan como gu√≠a para aprender el patr√≥n de los resultados que deseamos obtener [^4].

La idea de utilizar esta t√©cnica en nuestro proyecto surge, principalmente, al darnos cuenta que los modelos naturalmente no generan preguntas de verdadero o falso. Entonces, necesit√°bamos proporcionarles ejemplos para incentivarlos a generarlas.

Un ejemplo de entrada que le proporcionamos al modelo usando este enfoque es el siguiente:

```python
prompt = (
    f"En base al siguiente texto:\n\n{article}\n\n"
    "Responde las siguientes 10 preguntas:\n"
    "1. ¬øEn qu√© fecha naci√≥ Alan Turing?\n"
    "2. ¬øD√≥nde naci√≥ Alan Turing?\n"
    "3. La m√°quina sobre la cual trabaj√≥ durante la guerra se llam√≥ m√°quina Enigma. ¬øVerdadero o Falso?\n"
    "4. Se estima que el trabajo de Turing durante la guerra acort√≥ la misma entre 2 y 4 a√±os. ¬øVerdadero o Falso?\n"
)
```

### System prompts

*System prompts* son un conjunto de instrucciones, gu√≠as e informaci√≥n acerca del contexto que se le provee al modelo antes de interactuar con las consultas del usuario [^3].

El uso de modelos base para la generaci√≥n de preguntas presenta varias limitaciones. En primer lugar, no permite especificar de manera directa la cantidad y el tipo de preguntas a generar. Adem√°s, requieren estrategias m√°s avanzadas de ingenier√≠a de *prompts* para guiar de forma adecuada la generaci√≥n de las preguntas, lo que implica un mayor esfuerzo en el dise√±o de instrucciones √≥ptimas. Por estos motivos, se decidi√≥ optar por un modelo instruido como `meta-llama/Meta-Llama-3.1-8B-Instruct`, lo que tambi√©n tendr√° un impacto positivo en el *fine tuning*, ya que nos permite enfocarnos en mejorar el desempe√±o en la tarea espec√≠fica, sin la necesidad de ense√±arle al modelo a interpetar las instrucciones desde cero.

El cambio de modelo a `"meta-llama/Llama-3.1-8B-Instruct"` nos permiti√≥ aplicar esta t√©cnica en nuestro proyecto, debido a que cuenta con ciertos roles, entre los cuales destacamos *system* y *user*. El rol *system* sirve para establecer el contexto de una conversaci√≥n o proveerle al modelo informaci√≥n necesaria para ayudarle a responder efectivamente, mientras que el rol *user* representa la entrada del usuario [^5].

El siguiente fragmento de c√≥digo muestra c√≥mo definimos los roles `system`  y `user` en nuestras consultas al modelo:

```python
prompt = [
    {
        "role": "system",
        "content": (
            "Eres un generador de preguntas de lectura comprensiva. "
            "Tu tarea es crear preguntas basadas en un texto dado, "
            "sin incluir respuestas ni opciones. Solo debes generar preguntas, "
            "en las cuales deber√°s incluir preguntas de tipo verdadero o falso."
        ),
    },
    {
        "role": "user",
        "content": (
            "Genera exactamente 10 preguntas, de las cuales 5 deben ser de tipo "
            "verdadero o falso, sin incluir las respuestas ni opciones, sobre el siguiente texto:\n\n"
            f"{article}"
        ),
    },
]
```

### Cuantizaci√≥n
La cuantizaci√≥n es una t√©cnica que nos permite comprimir modelos extensos de lenguaje representando sus pesos y activadores, los cuales est√°n t√≠picamente en n√∫meros flotantes de 32-bits de precisi√≥n, con valores de menor precisi√≥n, usualmente 8-bits o 4-bits. La reducci√≥n en el n√∫mero de bits provoca una gran disminuci√≥n en el tama√±o del modelo, por lo que consumen menos memoria, requieren menos espacio de guardado y son m√°s r√°pidos de aplicarle *fine tuning* [^6][^7].

![imagen](https://i.imgur.com/3WSwSJF.png)

El modelo `"meta-llama/Llama-3.1-8B-Instruct"` requiere disponer de al menos 16‚ÄØGB de RAM [^8]. Sin embargo, dado que nuestro entorno de ejecuci√≥n en Google Colab solo ofrec√≠a 15‚ÄØGB de RAM, se hizo necesario aplicar cuantizaci√≥n para reducir el modelo. Para ello, implementamos la t√©cnica NF4 (*4-bit Normal Float*), la cual cuantifica los pesos a 4 bits, permiti√©ndonos ejecutar y evaluar distintos modelos (como Llama2-7B, Llama3.1-8B y Llama3.1-8B-Instruct) pese a las limitaciones de recursos del entorno.

Adem√°s, un art√≠culo de Hugging Face [^9], nos permiti√≥ saber antes de entrenar nuestro modelo que ser√≠a posible hacerlo en este entorno limitado usando GC (*gradient checkpointing*) y *nested quantization*.

### Fine tuning

*Fine tuning* es el proceso de ajustar los par√°metros de un modelo extenso de lenguaje pre-entrenado para una tarea o dominio espec√≠fico a trav√©s de un conjunto de datos. Esto dado que, los grandes modelos poseen un conocimiento amplio pero carecen de especializaci√≥n en √°reas espec√≠ficas, por lo que esta t√©cnica ataca ese problema haciendo a los modelos m√°s acertados y efectivos para ciertos dominios o aplicaciones [^10][^11].

*Supervised Fine Tuning* es un m√©todo para mejorar y personalizar modelos pre-entrenados, la cual consiste en volver a entrenar estos modelos en un conjunto de datos etiquetado, compuesto por las instrucciones y las respuestas que se desean. Las tres t√©cnicas m√°s populares para aplicar SFT son *full fine-tuning*, *LoRA* y *QLoRA*.

![imagen](https://i.imgur.com/IG9s8Yk.png)

*QLoRA (Quantization-aware Low-Rank Adaptation)* es una t√©cnica que en lugar de entrenar el modelo entero, congela los pesos e introduce peque√±os adaptadores denominados *low-rank matrices* lo que le permite reducir mucho el n√∫mero de par√°metros a entrenar, reduciendo el uso de la memoria y el tiempo de entrenamiento respecto al *full fine-tuning*. Esta t√©cnica es ideal para entornos donde la memoria GPU es limitada y por eso fue la elegida para entrenar nuestro modelo. La siguiente imagen ilustra la reducci√≥n de par√°metros entrenables que esta t√©cnica realiz√≥ en nuestro caso.

![](https://i.imgur.com/vqrHSB7.png)

El conjunto de datos que seleccionamos para esta t√©cnica fue *Spanish Question Answering Corpus* (SQAC) [^12], que se compone de preguntas-respuestas extractivas en espa√±ol. Sin embargo, lo consideramos ideal para la tarea de este proyecto, ya que posee contextos extra√≠dos de art√≠culos de Wikipedia, noticias de WikiNews y la secci√≥n espa√±ola de AnCora corpus que es una mezcla de diferentes fuentes de noticias y literatura. Adem√°s, no posee preguntas sin respuestas y tanto las preguntas como las respuestas fueron anotadas por hablantes nativos de espa√±ol con estudios universitarios [^13]. 

Ahora bien, a este conjunto de datos fue necesario realizarle un pre-procesamiento donde solamente nos quedamos con contextos provenientes de la secci√≥n "Biograf√≠as" de art√≠culos de Wikipedia. Posterior a eso, nos quedamos con las columnas `context` y `question` para, a partir de ellas, armar una sola columna (`text`) representando una conversaci√≥n entre un usuario que provee un contexto y solicita una cierta cantidad de preguntas, y un asistente que genera las preguntas a partir de ese contexto.

## Librer√≠as exploradas y elecciones

### Transformers

![](https://i.imgur.com/jSFvf9J.png)

Transformers es una librer√≠a mantenida por Hugging Face y su comunidad, la cual proporciona APIs para descargar y entrenar modelos pre-entrenados. El uso de modelos pre-entrenados reduce los costos de c√≥mputo y tiempo que conllevar√≠an entrenar un modelo de cero [^14]. Seleccionamos esta librer√≠a tras elegir el modelo, la cual era mencionada en la p√°gina de Meta donde lo presentaban [^15]. 

La elecci√≥n de esta librer√≠a se debi√≥ a diversos motivos, entre ellos podemos destacar el f√°cil acceso a modelos pre-entrenados, ya que mediante esta pudimos acceder a diversos modelos de forma simple, acelerando el desarrollo de la prueba de concepto. Adem√°s, la librer√≠a cuenta con mucha documentaci√≥n [^16], tutoriales y una comunidad activa en sus foros, lo que facilit√≥ el aprendizaje y la resoluci√≥n de problemas que se presentaron durante el desarrollo. Otro motivo para su elecci√≥n, fue que ofrece varias APIs para distintos niveles de conocimientos, lo que permiti√≥ adaptar el desarrollo a medida que aprend√≠amos. Asimismo, su sencillez nos permiti√≥ realizar inferencia, t√©cnicas de *prompting* y ajustar modelos pre-entrenados en unas cuantas l√≠neas de c√≥digo, ahorr√°ndonos tiempo y recursos [^17].

### PyTorch

PyTorch es una librer√≠a optimizada de tensores para aprendizaje profundo usando GPUs y CPUs [^18]. Los tensores, matem√°ticamente, son matrices multi-dimensionales y en este contexto, representan informaci√≥n aparentemente no num√©rica, como pueden ser textos o im√°genes.

En el aprendizaje profundo se manejan grandes vol√∫menes de datos, esto sumado  a que √≠bamos a estar trabajando en un entorno con GPU, nos llevo a seleccionar PyTorch para manejar tensores dado a que permite realizar calculos de estos tanto en CPU como en GPU (contrario a los *ndarrays* de NumPy) [^19], y adem√°s es compatible con Transformers de Hugging Face para la familia de modelos Llama 3 [^20].

### Bitsandbytes

Los modelos extensos de lenguaje consumen muchos recursos para ser ejecutados y entrenados, por lo tanto el acceso a ellos es un desaf√≠o para los usuarios [^9]. Bitsandbytes es una librer√≠a que permite la cuantizaci√≥n en k-bits usando PyTorch, haciendo a los modelos extensos de lenguaje m√°s accecibles [^21].

La elecci√≥n de esta librer√≠a se debi√≥ principalmente a que permit√≠a la cuantizaci√≥n en 4-bits usando NF4 y, tal como se menciona en su documentaci√≥n, era ideal para aplicar la t√©cnica de QLoRA en el *fine tuning* del modelo.

### Wikipedia-API

`Wikipedia-API` es un *wrapper* sencillo de usar para las API de Wikipedia que permite extraer textos, secciones, links, categor√≠as, traducciones y dem√°s [^22]. Hab√≠amos explorado otro *wrapper* incluso m√°s simple [^23], pero cuando necesitamos procesar los art√≠culos antes de pas√°rselos al modelo para hacer inferencia, comenzamos a tener dificultades para eliminar secciones no relevantes correctamente.

Usamos el *wrapper* seleccionado para obtener los art√≠culos sobre los cuales luego le solicit√°bamos al modelo realizar las preguntas.

### Datasets y Pandas
Datasets es una librer√≠a para acceder y compartir f√°cilmente conjuntos de datos para diversas tareas, entre ellas el procesamiento de lenguaje natural [^24].

Elegimos utilizar esta librer√≠a debido a que nos prove√≠a acceso al conjunto de datos que elegimos, es f√°cil de utilizar y provee m√©todos los cuales son poderosos para el procesamiento de datos (como `map` y `filter`). Adem√°s, est√° dise√±ada para manejar de forma eficiente grandes conjuntos de datos, evitando cargarlos directamente en la memoria RAM usando Apache Arrow [^25].

Sin embargo, no fue posible sacarle provecho al m√°ximo a esta elecci√≥n. En un momento tomamos la decisi√≥n de convertir nuestro conjunto de datos en un DataFrame de Pandas [^26], el motivo fue que nos facilitaba agrupar las preguntas por contextos mediante el m√©todo`groupby`. No obstante, esto no represent√≥ ninguna dificultad, ya que ambas librer√≠as cuentan con m√©todos para convertir entre DataFrames y Datasets.

### TRL

TRL provee herramientas para entrenar modelos de lenguaje que posean la arquiectura de Tranformers mediante aprendizaje de refuerzo [^27].

La elecci√≥n de esta libreria fue crucial para aplicar *supervised fine tuning* [^28], la cual es una t√©cnica que consiste en adaptar un modelo extenso de lenguaje pre-entrenado a una tarea espec√≠fica usando datos etiquetados. En nuestro caso, los datos etiquetados estaban conformados por preguntas del conjunto de datos SQAC y TRL mediante la clase `SFTTrainer` nos facilit√≥ poner esta t√©cnica en pr√°ctica en nuestro proyecto.

Otros motivos de la elecci√≥n de esta libreria fueron su integraci√≥n con la librer√≠a Transformers de Hugging Face, la amplia documentaci√≥n que posee, el soporte con PEFT, libreria cuya importancia detallaremos en la pr√≥xima secci√≥n.

### PEFT
PEFT es una librer√≠a que nos permite adaptar de manera eficiente modelos extensos de lenguaje sin entrenar todo los p√°rametros del modelo (*full fine-tuning*). En su lugar, los m√©todos de PEFT solo entrenan una peque√±a cantidad de par√°metros extra, reduciendo los costos de entrenamiento en t√©rminos recursos computacionales y de almacenamiento, con un rendimiento comparable a haber entrenado el modelo completo [^29].

Optamos por esta librer√≠a debido a su integraci√≥n con Transformers  las herramientas que proporciona para configurar y aplicar QLoRA de manera sencilla.

### Wandb

Es una librer√≠a de la plataforma *Weights & Biases* la cual ofrece herramientas para entrenar modelos, afinarlos y aprovechar los modelos fundamentales [^30].

Esta plataforma nos permit√≥ llevar un seguimiento de los hiper-par√°metros  utilizandos para entrenar el modelo junto con un monitoreo en tiempo real del comportamiento del mismo durante el entrenamiento. Asimismo, ofreci√≥ un registro de varias m√©tricas del modelo como el *training loss* mediante gr√°ficos muy √∫tiles. Todo esto, adem√°s facilit√≥ la comparaci√≥n de diferentes entrenamientos realizados.

### Unsloth

Unsloth es una herramienta que promete entrenar modelos extensos de lenguaje m√°s r√°pido y con menos recursos, sin p√©rdida de *accuracy* [^31]. En nuestro caso, intentamos usarlo para inferencia primero. Como resultado, obtuvimos una velocidad de inferencia mayor comparada a la realizada sin el uso de esta herramienta, pero notamos una degradaci√≥n en la calidad de respuesta usando contextos largos (alrededor de 15.000 tokens), generando res√∫menes en lugar de preguntas sobre del texto solicitado, por lo que optamos por no seguirla utilizando.

Nuestra hip√≥tesis sobre esta degradaci√≥n se basa en la forma en la que Unsloth maneja los contextos a trav√©s de *RoPE Scaling* [^32]. Consideramos que, al procesar contextos de este tama√±o, el modelo podr√≠a perder la capacidad de retener la instrucci√≥n solicitada, tendiendo en su lugar a realizar la instrucci√≥n m√°s com√∫n asociada con dicha longitud de contexto.

## Bases de c√≥digo exploradas

Dado a que era nuestra primera vez trabajando en un proyecto sobre el procesamiento de lenguaje natural, fue necesario explorar diferentes bases de c√≥digo para aprender las t√©cnicas m√°s utilizadas, las diferentes partes en el desarrollo y para mantener nuestro c√≥digo simple y f√°cil de leer pero modernizado, ya que varias de las librerias utilizadas en el proyecto se actualizan de forma regular.



| Nombre                                                                                  | Repositorio                                                                                                      | Prop√≥sito                                                                          | Aspectos clave                                                                              | Motivo de an√°lisis                                                                                                                              |
| --------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------- |
|    ü§ó **Tokenizers - Hugging Face**                                                                                       |                                                                                             [huggingface/tokenizers](https://github.com/huggingface/tokenizers)                     | Pre-procesameinto de datos, inferencia y entrenamiento.                                                                                    |     Buena documentaci√≥n y muchos ejemplos.                                                                                        | Ofrece una implementaci√≥n optimizada para la tokenizaci√≥n r√°pida y eficiente, compatible con los modelos del Hub. |
| ü§ó **Datasets - Hugging Face**                                                          | [huggingface/datasets](https://github.com/huggingface/datasets)                                                  | Descarga y preprocesamiento de datos.                                              | Carga de conjunto de datos p√∫blicos de forma sencilla y pre-procesado eficiente.            | Ofrece c√≥digo para descargar conjuntos de datos y diferentes configuraciones para hacerlo.                                                      |
| ü§ó **TRL - Hugging Face**                                                               | [huggingface/trl](https://github.com/huggingface/trl)                                                            | *Fine tuning* usando aprendizaje supervisado.                                      | Soporte con PEFT, buena documentaci√≥n, eficiencia y `Trainers`.                             | Brinda implementaciones de entrenamientos, ejemplos de formatos de conjunto de datos utilizados para entrenar y de configuraci√≥n de par√°metros. |
| ü§ó **NLP Course - Hugging Face**                                                        | [huggingface/course](https://github.com/huggingface/course)                                                      | Inferencia de los distintos modelos probados y comprensi√≥n de conceptos claves.    | Aprendizaje guiado sobre el ecosistema de HuggingFace y conceptos clave de NLP.             | Instruirnos en el uso de `Transformers`, `Dataset` y `Tokenizers`.                                                                              |
| ü§ó **Transformers - Hugging Face**                                                      | [huggingface/transformers](https://github.com/huggingface/transformers)                                          | Implementaci√≥n de modelos LLM.                                                     | Compatibilidad con modelos Llama y comunidad activa.                                        | Base para hacer inferencia probando modelos mediante `pipeline`.                                                                                |
| üìñ **LLM Course**                                                                       | [mlabonne/llm-course](https://github.com/mlabonne/llm-course)                                                    | *Fine tuning* y pre-procesamiento de datos.                                        | Explicaciones detalladas, entrenamiento ultra-eficiente y c√≥digo f√°cil de leer.             | Aprender c√≥mo entrenar el modelo de manera eficiente, usando t√©cnicas de aprendizaje supervisado.                                               |
| ü§ó **PEFT - Hugging Face**                                                              | [huggingface/peft](https://github.com/huggingface/peft)                                                          | *Fine-tuning* usando QLoRA y valor de par√°metros para la aplicaci√≥n de la t√©cnica. | *Fine-tuning* eficiente y buena documentaci√≥n.                                              | Ofrece explicaci√≥n detallada acerca de los par√°metros para aplicar QLoRA y c√≥digo donde se aplica la t√©cnica.                                   |
| ‚ö° **Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA** | [colab/bnb-4bit-training](https://colab.research.google.com/drive/1VoYNfYDKcKRQRor98Zbf2-9VQTtGJ24k?usp=sharing) | *Fine tuning* de modelos mediante QLoRA y cuantizaci√≥n en NF4.                     | Uso del ecosistema de HuggingFace, *nested quantization*, NF4 y *Gradient checkpointing*.   | Ofrece una implementaci√≥n optimizada usando QLoRA y es compatible con HuggingFace.                                                              |
| ü¶• **Unsloth**                                                                          | [unslothai/unsloth](https://github.com/unslothai/unsloth)                                                        | Optimizaci√≥n para realizar inferencia y *fine tuning*.                             | Optimizaci√≥n de la memoria, compatibilidad con modelos Llama 3.1 y velocidad de inferencia. | Brinda c√≥digo para entrenar y hacer inferencia de modelos de la familia Llama 3, usando la herramienta Unsloth en un entorno de HuggingFace.    |
| üìö **Wikipedia-API**                                                                    | [project/wikipedia-API](https://pypi.org/project/Wikipedia-API/)                                                 | Obtenci√≥n de datos de Wikipedia para probar modelos.                               | Simpleza y facilidad para recuperar informaci√≥n de Wikipedia.                               | Ofrece c√≥digo sobre el uso del *wrapper* mediante casos de uso.                                                                                 |

## Problemas encontrados

En esta secci√≥n detallamos los principales desaf√≠os que surgieron durante el desarrollo del proyecto y las estrategias implementadas para resolverlos.

### Recursos y entorno de ejecuci√≥n

Comenzamos utilizando Google Colab en el proyecto debido a sus ventajas,  como la accesibilidad, colaboraci√≥n en tiempo real y disponibilidad de GPU gratuitas. Continuamos usando esta plataforma en cada etapa, incluyendo el *fine tuning*. Sin embargo, a medida que el proyecto avanzaba, los recursos ofrecidos por este entorno comenzaron a limitarnos, lo que nos hizo recurrir a las t√©cnicas de cuantizaci√≥n y optimizaci√≥n del rendimiento mencionadas anteriormente.

### Elecci√≥n del tipo de preguntas a generar

En primera instancia, probamos el desempe√±o de diferentes modelos en la terea de generar preguntas en base a un art√≠culo. No obstante, la comparaci√≥n entre las preguntas generadas cada vez fue m√°s dif√≠cil de hacer, lo que nos hizo pensar que nuestra forma de categorizar preguntas como "buenas" o "malas" carec√≠a de certeza, provocando incertidumbre al momento de decidir si la calidad de las preguntas generadas mejoraba. Para solucionarlo, definimos una serie de pautas acerca de la caracter√≠sticas que deseamos que las preguntas generadas por el modelo posean:
* *Tipos de preguntas:* los tipos de preguntas que se desean generar son de respuesta corta y sentencias a determinar en verdaderas o falsas. Adem√°s, se considerar√° que la respuestas a dichas preguntas est√©n presentes dentro del art√≠culo proporcionado.
* *No repetici√≥n:* deseamos diversidad de preguntas, por lo tanto, dos de la misma lista no deben ser ni id√©nticas ni demasiado similares. A continuaci√≥n, dejamos un ejemplo de preguntas similares que queremos evitar.
  ```
   Pregunta 1: ¬øQu√© hizo en el Siglo 19?
   Pregunta 2: ¬øQu√© hizo en el Siglo 20?
  ```
* *Naturalidad y fluidez:* las preguntas generadas deben evitar una estructura mecanizada, minimizando repeticiones innecesarias y mostrando una formulaci√≥n de preguntas similiar a las realizadas por una persona.
* *Completitud:* priorizaremos la relevancia de las preguntas pero tendremos en cuenta que sean abarcativas a los temas tratados en el articulo.
* *Orientadas al refuerzo de la comprensi√≥n:* las preguntas buscar√°n evaluar a quien las responda en su habilidad para identificar ideas principales y detalles espec√≠ficos (por ejemplo: fechas).

### Elecci√≥n del modelo

Definir el modelo y la versi√≥n a usar del mismo nos llevo m√°s tiempo del que esper√°bamos, ya que tuvimos que investigar y probar diferentes posibilidades.

Deb√≠amos buscar modelos que tengan soporte para la generaci√≥n de texto en espa√±ol. En dicha b√∫squeda, encontramos a la familia de modelos LLaMa en su versi√≥n 3.1 que son multiling√ºes [^33]. Para comprender la importancia de este punto, en la siguiente subsecci√≥n mostraremos una comparativa sencilla entre la capacidad de generaci√≥n de texto en espa√±ol del modelo `meta-llama/Llama-2-13b-hf`, que no tiene soporte oficial para el idioma, y `meta-llama/Llama-3.1-8B`.

Adem√°s, como mencionamos en la secci√≥n *system prompt*, terminamos eligiendo el modelo `meta-llama/Llama-3.1-8B-Instruct` ya que nos otorgaba m√°s facilidades para solicitarle instrucciones al modelo.

#### Comparativa: soporte para la generaci√≥n de texto en espa√±ol

La comparativa const√≥ en solicitarle a ambos modelos completar una sentencia que comenzaba con `"El perro es"`, con los mismos par√°metros de generaci√≥n, entre los cuales incluimos un n√∫mero para `max_new_tokens` para as√≠ evitar secuencias generadas que sean muy largas. La siguiente imagen es un ejemplo de los textos generados por ambos modelos.

![Compartiva](https://i.imgur.com/UsBXOZ6.png)

Como ya hemos mencionado a lo largo del art√≠culo, la elecci√≥n del modelo  `"meta-llama/Llama-3.1-8B-Instruct"` fue clave para el *prompting*, debido a que puede interpretar instrucciones y usar roles en su entrada para adaptar su salida [^34].

### Reducir el dominio

Los art√≠culos presentes en Wikipedia pueden tratarse de diversos temas, asimismo una pregunta puede ser menos relevante en un art√≠culo que en otro. Es por eso que decidimos reducir el dominio de los art√≠culos a solo biograf√≠as, sobre los cuales evaluaremos las preguntas generadas. De esta manera, ser√≠a m√°s f√°cil evaluar la calidad de las preguntas y detectar las limitaciones del modelo.

### Respuestas no solicitadas junto con las preguntas generadas

La tarea que le solicitamos al modelo siempre fue generar preguntas a partir de un determinado contexto. Uno de los problemas que surgieron con esta solicitud fue que el modelo tambi√©n generaba las respuestas de dichas preguntas, inclusive si le especific√°bamos expl√≠citamente que no lo haga, a trav√©s del rol `system`. A este problema pudimos solucionarlo parcialmente, mediante la t√©cnica de *few shot prompting* en conjunto con una mejora en la redacci√≥n de la solicitud suministrada realizando una m√°s corta y directa. Sin embargo, la soluci√≥n definitiva a este problema fue el *fine tuning*.

### Error en una URL del conjunto de datos

El conjunto de datos que decidimos utilizar est√° presente como `Dataset` en Hugging Face [^11], para facilitar su uso mediante su API. No obstante, el m√≥dulo encargado de descargarlo llamado `SQAC.py` posee un error en una URL que nos lleva a una p√°gina que no existe y lo cual no permite obtenerlo mediante el comando `load_dataset("PlanTL-GOB-ES/SQAC")`. Para solucionarlo, debimos descargar los archivos del conjunto de datos desde la p√°gina de HF y cambiar el valor de la variable `_URL` del archivo `SQAC.py` por `"https://huggingface.co/datasets/PlanTL-GOB-ES/SQAC/resolve/main/"`.

Este error lleva tiempo ah√≠, inclusive un usuario solicit√≥ un *pull request* con la soluci√≥n, sino que a√∫n no ha sido aceptada [^35].

### Familia de modelos LLama3 y los tokens de relleno

El tokenizador de la familia de modelos LLaMa, no poseen un `pad_token` definido, lo que es un problema para hacer inferencia, y tambi√©n, para aplicar *fine tuning* sobre ellos. Hemos investigado mucho al respecto, y es un hecho, la falta de informaci√≥n oficial sobre este tema provoca una gran confusi√≥n entre los usuarios [^36][^37]. La soluci√≥n que abordamos para este problema fue la que habitualmente se utiliza, definir el `pad_token` igual que el `eos_token` y mantener un tama√±o de *batch* de 1 durante el entrenamiento para evitar problemas debido al token de relleno.

### Formato del conjunto de datos

Adoptamos como primer enfoque implementar la t√©cnica de *fine tuning* sobre 1000 preguntas repartidas en 840 contextos diferentes del conjunto de datos SQAC. No obstante, en el entrenamiento del modelo pudimos notar que algo andaba mal debido a que el *training loss* no disminu√≠a, sino que oscilaba siempre entre los mismos valores, como se puede observar en el siguiente gr√°fico:

![Training loss sin cambios](https://i.imgur.com/wPDLTNc.pngg)

Al finalizar el entrenamiento y evaluar el modelo, observamos que la generaci√≥n de texto carec√≠a de coherencia y naturalidad. Adem√°s, el modelo tend√≠a a repetir las pocas preguntas que lograba formular y no inclu√≠a un token especial de terminaci√≥n, sino que entraba en un bucle generando contenido repetitivo (lo que suele ser un problema entre los usuarios [^38]).

Nuestra hip√≥tesis fue que el modelo no lograba captar el patr√≥n en los datos suministrados para lograr aprender de ellos. En una primera instancia, pensamos que era porque hab√≠a demasiados tipos de art√≠culos distintos en los fragmentos del conjunto de datos, por lo que probamos entrenar el modelo nuevamente, pero esta vez, filtrando aquellos contextos que provengan de la secci√≥n "Biograf√≠a" de alg√∫n art√≠culo, y entrenando el modelo con conversaciones formuladas a partir de las preguntas asociadas a tales contextos.

Cabe aclarar que un mismo contexto puede poseer varias preguntas asociadas, lo que provoca que se repita varias veces en el conjunto de datos (tantas veces como preguntas asociadas tenga), entonces fue necesario agrupar las preguntas por contexto para generar las conversaciones antes mencionadas.

![Comparacion training loss 1](https://i.imgur.com/w6N63VM.png)

Como podemos ver en la imagen anterior, este nuevo enfoque hizo que el *training loss* disminuyera. Sin embargo, los problemas de generaci√≥n segu√≠an sin ser solucionados. Por lo tanto, la causa del problema era otra y efectivamente, revisando la documentaci√≥n de las funciones que hab√≠amos usado para entrenar el modelo notamos que `SFTTrainer` ten√≠a un formato espec√≠fico para recibir el conjunto de datos de entrenamiento sin necesidad de ser pre-procesado directamente antes, que es lo est√°bamos haciendo mediante el `tokenizer` del modelo [^39]. Entonces, elegimos el formato conversacional el cual tiene la siguiente estructura:

```python
{"messages": [{"role": "system", "content": "You are helpful"}, {"role": "user", "content": "What's the capital of France?"}, {"role": "assistant", "content": "..."}]}
{"messages": [{"role": "system", "content": "You are helpful"}, {"role": "user", "content": "Who wrote 'Romeo and Juliet'?"}, {"role": "assistant", "content": "..."}]}
```
Con el cual, como podemos ver en la imagen a continuaci√≥n obtuvimos una mayor disminuci√≥n del *training loss* y adem√°s, el modelo entrenado genera un token especial para indicar cuando ha terminado.

![Comparacion training loss 2](https://i.imgur.com/ajH9UgV.png)


###  Evaluaci√≥n del progreso
Hasta ahora se realiz√≥ una evaluaci√≥n anecd√≥tica manual comparando los diferentes resultados obtenidos al cambiar las entradas y las t√©cnicas utilizadas. Se planea para un futuro investigar alternativas de evaluaci√≥n.

## Conclusiones
En esta secci√≥n abordaremos diferentes conclusiones que pudimos obtener de realizar este proyecto.

### Planificaci√≥n inicial y ejecuci√≥n efectiva

Nuestra falta de experiencia en el procesamiento de lenguaje natural mediante modelos extensos de lenguaje caus√≥ la omisi√≥n involuntaria de aspectos importantes durante la etapa de planificaci√≥n del proyecto, como la elecci√≥n de un modelo apropiado, cr√≠terios de evaluaci√≥n de los resultados obtenidos y hasta falta de precisi√≥n en los objetivos espec√≠ficos. Todos estos detalles, obstaculizaron el progreso y nos obligaron a regresar a la etapa de planificaci√≥n en diversas ocaciones a subsanar los errores cometidos.

La impericia mencionada en el p√°rrafo anterior ocacion√≥ tambi√©n una mala gesti√≥n del tiempo, donde tareas que pens√°bamos terminar r√°pidamente nos llevaron mucho m√°s tiempo de lo estipulado. De esta manera, terminamos aplicando cada t√©cnica planteada, con el fin de cumplir con todos los objetivos en el tiempo acordado, dejando en segundo plano la evaluaci√≥n de las mejoras obtenidas, lo que produjo una falta de comprensi√≥n en los impactos de cada una.

### Entorno de ejecuci√≥n limitado

Mencionamos en reiteradas ocaciones a lo largo de este informe que el entorno de ejecuci√≥n sobre el cual est√°bamos trabajando pose√≠a recursos limitados, forz√°ndonos a tomar ciertas decisiones para seguir trabajando sobre √©l. Realmente, trabajar sobre este entorno fue una decisi√≥n, ya que no era el √∫nico que disponiamos, sino que tambi√©n cont√°bamos con acceso a una m√°quina del Centro de Computaci√≥n de Alto Desempe√±o de la UNC (CCAD), con mejores especificaciones.

Consideramos que esta decisi√≥n ocacion√≥ que debamos invertir demasiado tiempo en investigar t√©cnicas de optimizaci√≥n avanzadas, el cual  nos hubi√©semos ahorrado eligiendo el otro entorno. Sin embargo, debemos mencionar que trabajar en un entorno limitado nos hizo descrubrir que este es un problema usual en esta √°rea y nos di√≥ a conocer t√©cnicas que la comunidad emplea para solucionarlo.

### Conclusiones sobre t√©cnicas utilizadas

Mediante la t√©cnica *zero-shot* aplicada al modelo base `meta-llama/Llama-3.1-8B` obtuvimos preguntas las cuales, en su mayoria, ni siquiera cumpl√≠an con el  tipo especificado, incluso generaba preguntas repetidas, muy similares entre s√≠ y hasta inclu√≠a respuestas. Adem√°s, era complicado indicarle al modelo el n√∫mero de preguntas a generar, por lo que a veces generaba m√°s o menos, dependiendo del par√°metro de detenci√≥n que le pas√°bamos. Sobre este mismo modelo, aplicamos *few-shot* con el fin de obtener preguntas que se adecuaran a los tipos especificados y lo conseguimos, pero segu√≠amos con las mismas deficiencias que la t√©cnica anterior, entonces decidimos cambiar el modelo.

Aplicando *system prompts* y *few shot* al modelo `meta-llama/Llama-3.1-8B` consegu√≠mos mejorar notablemente la calidad de las preguntas generadas y era posible especificar, sin problemas, la cantidad de preguntas a generar, es m√°s, pod√≠amos pedir cu√°ntas preguntas de cada tipo quer√≠amos. El modelo, normalmente, generaba preguntas dentro de los tipos detallados, aunque, sol√≠a generar preguntas que ten√≠an respuestas abiertas, no orientadas al refuerzo de la compreni√≥n o incluyendo las respuestas. *Few-shot*, en este caso, lo aplicamos pasando otros art√≠culos junto con preguntas de ejemplo que se pod√≠an realizar sobre los mismos antes de la instrucci√≥n en un formato de conversaci√≥n. 

Finalmente con *fine tuning* solucionamos el problema de preguntas que inclu√≠an respuestas, aunque invertimos bastante tiempo en elegir el conjunto de datos apropiado, pre-procesarlo y entrenar el modelo sobre √©l. El modelo resultante genera preguntas que cumplen con el tipo de respuesta corta, poseen  naturalidad, fluidez y cuya respuesta est√° presente en el texto proporcionado, todo esto debido a la naturaleza del conjunto de datos.

Como conclusi√≥n de esta secci√≥n consideramos que para esta tarea, realizar *few-shot* con un *prompt* bien estructurado es m√°s que suficiente. El modelo obtenido mediante *fine tuning* conlleva mucho esfuerzo pero sigue teniendo limitaciones importantes, por ejemplo al no tener preguntas de verdadero o falso en su conjunto de datos tiende a no generarlas.

### Trabajo Futuro

Si bien logramos mejorar la generaci√≥n de preguntas mediante *fine tuning* y *few shot prompting*, quedan desaf√≠os que abordar como la diversificaci√≥n de resultados, ya que recordemos que lo trabajado en este proyecto fue enfocado solamente a biograf√≠as, nos gustar√≠a poder ampliar este trabajo a otros tipos de texto. Asimismo, nos gustar√≠a mejorar la evaluaci√≥n las preguntas generadas y la exploraci√≥n de otras limitaciones.


****************************************

### Bibliograf√≠a

Medina, Julia A. (2024). *Exploraci√≥n de t√©cnicas de
prompt-programming para sistemas
de recomendaci√≥n*. Universidad Nacional de C√≥rdoba.

### Referencias
[^1]: https://urjconline.atavist.com/2023/06/29/pregunta-tipo-respuesta-corta/.
[^2]: https://www.promptingguide.ai/techniques/zeroshot.
[^3]: https://promptengineering.org/system-prompts-in-large-language-models.
[^4]: https://www.promptingguide.ai/techniques/fewshot.
[^5]: https://huggingface.co/blog/llama31#how-to-prompt-llama-31.
[^6]: https://www.llama.com/docs/how-to-guides/quantization/.
[^7]: https://www.datacamp.com/tutorial/quantization-for-large-language-models.
[^8]:  https://llamaimodel.com/requirements/.
[^9]: https://huggingface.co/blog/4bit-transformers-bitsandbytes.
[^10]: https://toloka.ai/blog/base-llm-vs-instruction-tuned-llm/.
[^11]: https://www.turing.com/resources/finetuning-large-language-models#primary-fine-tuning-approaches
[^12]: https://huggingface.co/datasets/PlanTL-GOB-ES/SQAC.
[^13]: https://portal.odesia.uned.es/dataset/sqac-es.
[^14]: https://huggingface.co/docs/transformers/v4.49.0/es/index.
[^15]: https://about.fb.com/ltam/news/2024/07/presentamos-llama-3-1-nuestro-modelo-de-lenguaje-a-gran-escala-mas-capaz-hasta-la-fecha/.
[^16]: https://huggingface.co/docs.
[^17]: https://huggingface.co/docs/transformers/philosophy.
[^18]: https://pytorch.org/docs/stable/index.html.
[^19]: https://www.ibm.com/es-es/topics/pytorch.
[^20]: https://huggingface.co/docs/transformers/index#supported-models-and-frameworks.
[^21]: https://huggingface.co/docs/bitsandbytes/index.
[^22]: https://wikipedia-api.readthedocs.io/en/latest/?badge=latest.
[^23]: https://wikipedia.readthedocs.io/en/latest/code.html#module-wikipedia.exceptions.
[^24]: https://huggingface.co/docs/datasets/index.
[^25]: https://huggingface.co/learn/nlp-course/en/chapter5/4.
[^26]: https://pandas.pydata.org/.
[^27]: https://huggingface.co/docs/trl/index.
[^28]: https://en.wikipedia.org/wiki/Supervised_learning.
[^29]: https://huggingface.co/docs/peft/index.
[^30]: https://docs.wandb.ai/guides/#what-is-wb.
[^31]: https://unsloth.ai/introducing.
[^32]: https://huggingface.co/blog/unsloth-trl.
[^33]: https://ai.meta.com/blog/meta-llama-3-1/.
[^34]: https://www.llama.com/docs/how-to-guides/prompting.
[^35]: https://huggingface.co/datasets/PlanTL-GOB-ES/SQAC/discussions/4.
[^36]: https://discuss.huggingface.co/t/llama-pad-token/48001.
[^37]: https://discuss.huggingface.co/t/how-to-set-the-pad-token-for-meta-llama-llama-3-models/103418/6.
[^38]: https://huggingface.co/meta-llama/Meta-Llama-3-8B/discussions/60.
[^39]: https://huggingface.co/docs/trl/v0.11.1/en/sft_trainer#dataset-format-support.